% errors reported by:
% Ross Kendle (1 Augugust 2010)
% Chris Grompanopoulos <grobano@gmail.com>  (22 January 2013)
% Jingguo Yao (29 May 2014, 6 June 2014)
% David Clark (7 Jan 2015)
% Kavinda Wewegama (18 Feb 2015)
% Martin Riener
% Grant Slatton (7 Mar 2015)
% Piyush Bansal (29 May 2015)

% hypermode.tex controls chapter numbering
% versions.tex controls version numbers

% Comments on a problem in concurrent programming control 
% Harris Hyman  
% January 1966   Communications of the ACM , Volume 9 Issue 1  

% VERSION HISTORY

% Version of ?? 2015
% Rewrote Section 8 (Bounded Channel and Bounded Buffer)
% Modified Section 18 (now titled "Variable Hiding and Auxiliary Variables")
% Modified Section 7.9 (Mutual Exclusion in Modern Programs)

% Version of 14 July 2012
% - Began the Principles track with Section 5 on mutual exclusion
% - Modified the proof track to reflect the fact that TLAPS's
%   SimpleArithmetic back end has been replaced by SMT backends.
% - Corrected a serious error in the Simple PlusCal Reduction Theorem
%   of Section 4.8.
% 
% Version of 12 April 2012
% - Added section 16.4 on LAMBDA, and 
%   changed example of a higher-order op in section 16.2.1. 
% - Adding Section 8 on CandidateRanking spec
% - Began a new Section 19, debugging with TLC
% - Wrote section 14.4 on the SUBSET and UNION operators
%   in the Sets chapter.

\documentclass[fleqn,leqno]{article}
\usepackage{hypertlabook}
\usepackage{graphpap}

%%%%% Added by hengxin (hfwei@nju.edu.cn) %%%%%
\input{trans-dict}

\usepackage{xeCJK}
\usepackage{fontspec}
\usepackage{xcolor}
\usepackage{savesym}	% to resolve conflicts between packages
\savesymbol{tilde}
\usepackage{comment}	
\includecomment{ch}
\excludecomment{en}
\newcommand{\fixme}[1]{\textcolor{purple}{FIXME: #1}}
%%%%% Added by hengxin (hfwei@nju.edu.cn) %%%%%

\makeindex

%%% Hack for showing index elements.  Will cause errors if
%%  run on entire file
%%
% \let\mmath=\ensuremath
% \def\icmd#1{\csname#1\endcsname}
% \renewcommand{\tindex}[2]{\marginpar{\red #2 (#1)}}
% \renewcommand{\ctindex}[3]{\marginpar{\red #2 (#1)}}


%\usepackage{showidx}

\pdftitle{Principles and Specification Tracks}  
\title{\bf The Principles and Specification Tracks}
\file{main}
\author{Leslie Lamport}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%% Emacs macros %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%&1&\vspace{-#\baselineskip}%&
%&i&\tindex{#}{}%&
%&j&\ctindex{#}{}{}%&
%&2&\begin{twocols}
%\midcol
%\verb|#|
%\end{twocols}
%&
%&q&"#"&
%&"&"#"& "
%&c&\textsc{#}& 
%&C&\ctindex{#}{@\icmd{textsc}{}}{}&
%&f&\textsf{#}& 
%&t&\texttt{#}& 
%&b&\textbf{#}& 
%&k&\kwd{#}& 
%&v&\verb|#|&  
%&p&\documentclass[fleqn,leqno]{article}
%\usepackage{hypertlabook}
%\pdftitle{}
%\begin{popup}
%\subsection*{#}
%
%\end{popup}
%\makepopup&
%&a&\documentclass[fleqn,leqno]{article}
%\usepackage{hypertlabook}
%\pdftitle{ASCII Text}
%\fixverbatim
%\begin{popup}
%\begin{verbatim*}
%#
%\end{verbatim*}
%\end{popup}
%\makeasciipopup&
%&u&\documentclass[fleqn,leqno]{article}
%\usepackage{hypertlabook}
%\pdftitle{}
%\setpopup{30}
%\begin  {document}
%#
%\end  {document}&

%&o&\ov{#}& 
%&e&\ENABLED#& 
%&U&$\mathcal{U}$#& 
%&m&\mathcal{#}&
%&g&\gssub{#}{}&
%&h&\gssubvars{#}&
%&l&\sref{main}{\xlink{main:}}{Section~\xref{main:}}&
%&r&\rd{#}&
%&F&\MF#&
%&G&\MG#& 
%&T&\tlabox{#}&
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\contentsname}{The \emph{Principles} and \emph{Specification} 
       Tracks\protect\target{top}}

% PROOF
\beforePfSpace{0pt}
\afterPfSpace{0pt}
\interStepSpace{0pt}

% State commands

% DieHard: [big = #1, small = #2]
\newcommand{\dhstate}[2]{\left[\!\begin{array}{l@{\,\,}c@{\,\,}c}
                        big & = & #1 \\ small & = & #2
                        \end{array}\!\right]}

%Euclid: [x = #1, y = #2, pc = #3]
\newcommand{\pestate}[3]{\left[\!\begin{array}{l@{\,\,}c@{\,\,}c}
                        x & = & #1 \\ y & = & #2 \\ pc & = & #3
                        \end{array}\!\right]}

%Alternation: [b = #1, box = #2]
\newcommand{\altstate}[2]{\left[\!\begin{array}{l@{\,\,}c@{\,\,}c}
                        b & = & #1 \\ box & = & #2
                        \end{array}\!\right]}

% Handshake : [p = #1, c = #2, box = #3, bBar = #4, boxBar = #3]
\newcommand{\hdskstate}[4]{\left[\!\begin{array}{l@{\,\,}c@{\,\,}c}
                        p & = & #1 \\ c & = & #2 \\ box & = & #3 \\
                        \ov{b} & = & #4 \\ \ov{box} & = & #3
                        \end{array}\!\right]}

% 2-step Handshake : [p = #1, c = #2, box = #3, pc = 0 :> #4 @@ 1 :> #5]
\newcommand{\hdskkstate}[5]{\left[\!\begin{array}{l@{\,\,}c@{\,\,}l}
                        p & = & #1 \\ c & = & #2 \\ box & = & #3 \\
                        pc & = & 0 :> #4\,@\!@ \\ & & 1:>#5 
                        \end{array}\!\right]}


\begin{document}

% \thispagestyle{web}
%\maketitle
\target{contents}
\showversions
\tableofcontents
\hideversions
\vfill
\newpage
\vspace*{-\baselineskip}

\input{sections/intro}
\input{sections/clock}
\input{sections/diehard}
\input{sections/euclid}

\begin{comment}
%try

\newpage
\vspace*{-\baselineskip}

\section{The Generalized Die Hard Problem}  \xlabel{sec:die-harder}

We now generalize the \emph{Die Hard} problem of 
  \lref{\xlink{sec:diehard}}{Section~\xref{sec:diehard}}
to the problem of obtaining an arbitrary quantity of water with an
arbitrary collection of jugs.  The main purpose of this example is to
introduce the use of functions.

\subsection{The PlusCal Representation} 
Open a new spec named $DieHarder$ and add an \textsc{extends}
statement to import the $Integers$ module.  As in the original
$DieHard$ spec, we'll need the operator $Min$, where $Min(m,n)$ is the
smaller of the two numbers $m$ and $n$.  So, add its definition to the module.
\begin{twocols}
\begin{notla}
Min(m,n) == IF m < n THEN m ELSE n
\end{notla}
\begin{tlatex}
\@x{ Min ( m ,\, n ) \.{\defeq} {\IF} m \.{<} n \.{\THEN} m \.{\ELSE} n}%
\end{tlatex}
\midcol
\verb|Min(m,n) == IF m < n THEN m ELSE n|
\end{twocols}
A specification of the general Die Hard problem requires two constant
parameters:
\begin{describe}{$Capacity\;$}
\item[$Jugs$] The set of jugs.

\item[$Capacity\;$] A value that describes the capacity of each jug.
\end{describe}
For later use, we also declare the constant $Goal$, which will
represent the number of gallons of water that our generalized heros
must obtain.  So, add the following declaration to the $DieHarder$
module:%
   \marginpar[.7]{\popref{sections}{Splitting a module into sections.}}
\begin{twocols}
$\CONSTANTS Goal, \ Jugs, \ Capacity$
\midcol
\verb|CONSTANTS Goal, Jugs, Capacity|
\end{twocols}
%
We let $Capacity$ describe the capacities of the jugs by making it a
  \tindex{1}{function}%
function.  
%
Mathematical functions appear in programming languages,
where they are called 
  \ctindex{1}{function!versus array}{fcn-vs-array}%
  \tindex{1}{array}%
\emph{arrays}.  What programming languages call
  \tindex{1}{index set}%%%
the \emph{index set} of an array is known to mathematicians as the
  \tindex{1}{domain}%
  \ctindex{1}{function!domain of}{function-domain}%
\emph{domain} of a function.  However, while programming languages
  \marginpar[3.5]{In the C language, the index set of an array can only be a
  set of the form $0\dd n$ for some integer $n$.  Too many language
  designers have copied this limitation of C.}%
limit what kind of set can be the index set of an array, mathematics
allows a function to have any set as its domain---including an
infinite set.  Programmers think of an array $A$ as a collection of
``containers'', one for each element in its index set; they think of
$A[i]$ as the contents of $A$'s container $i$.  Mathematicians think
of a function $A$ as a rule that assigns to each element $i$ in its
domain a value $A(i)$.  \tlaplus\ uses the notation of programmers in
writing the value of a function $A$ applied to an element $i$ as
$A[i]$ rather than as $A(i)$.  In all other ways, I will use the
language of mathematicians, writing about functions rather than
arrays.

The constant $Capacity$ will be a function whose domain is the set
$Jugs$ of all jugs.  For each jug $j$, the value of $Capacity[j]$
should be a positive integer---that is, an element of $Nat :\: \{0\}$.
We say that $Capacity$ is a \emph{function from} $Jugs$ \emph{to}
$Nat :\: \{0\}$.  The set of all such function is written
 $[Jugs -> Nat :\: \{0\}]$.  We obviously want $Goal$ to be a natural
number.  We therefore add the following ``type assumption'' to the
module:
\begin{twocols}
\begin{notla}
ASSUME /\ Goal \in Nat
       /\ Capacity \in [Jugs -> Nat \ {0}]
\end{notla}
\begin{tlatex}
\@x{ {\ASSUME} \.{\land} Goal \.{\in} Nat}%
 \@x{\@s{38.24} \.{\land} Capacity \.{\in} [ Jugs \.{\rightarrow} Nat
 \.{\,\backslash\,} \{ 0 \} ]}%
\end{tlatex}
\midcol
\begin{verbatim*}
ASSUME /\ Goal \in Nat
       /\ Capacity \in [Jugs -> Nat \ {0}]
\end{verbatim*}
\end{twocols}
We will specify the system of jugs and water in PlusCal.  The system's
state is described by the contents of the jugs.  We represent it with
a variable $injug$, where $injug[j]$ is the number of gallons of water
in jug~$j$.  The declaration of variable $injug$ must specify its
initial value.  Initially, $injug[j]$ equals 0 for each jug~$j$.  In
other words, initially $injug$ equals a function with domain $Jugs$
that assigns~0 to every element in its domain.  Mathematics provides
no standard notation for writing this function.  It is written in
\tlaplus\ as $[j \in Jugs |-> 0]$.  In general, the expression
  \[ [v \in S |-> e(v)] \]
equals a function $f$ with domain $S$ such that $f[v]=e(v)$ for every
$v$ in $S$.  Thus, the algorithm begins
\begin{display}
\begin{nopcal}
--algorithm DieHarder {
  variable injug = [j \in Jugs |-> 0] ;
\end{nopcal}
\begin{tlatex}
\@x{ {\p@mmalgorithm} DieHarder \ {\p@lbrace}}%
 \@x{\@s{8.2} {\p@variable} injug \.{=} [ j \.{\in} Jugs \.{\mapsto} 0 ]
 {\p@semicolon}}%
\end{tlatex}
\end{display}
Had we wanted all the jugs to be full in the initial state, we would have
declared the initial value of $injug$ to be
 $[j \in Jugs |-> Capacity[j]]$.

From \popref{pdiehard}{our original $DieHard$ algorithm}, we see that 
the body of algorithm $DieHarder$ should be a 
  \textbf{while (}\TRUE\textbf{)}
loop whose body is an \textbf{either}/\textbf{or} statement that
permits three different actions:
\begin{itemize}
\item Filling some jug $j$.

\item Emptying some jug $j$.

\item Pouring from some jug $j$ to another jug $k$.
\end{itemize}
It should be clear how to express the first two actions as PlusCal
statements:
\vspace{-.5em}
\begin{twocols}
\begin{nopcal}
with (j \in Jugs) \* fill jug j
  { injug[j] := Capacity[j] }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@with} {\p@lparen} j \.{\in} Jugs {\p@rparen}}%
\@y{%
  \colorbox{gray}{fill jug j}
}%
\@xx{}%
\@x{\@s{8.2} {\p@lbrace} injug [ j ] \.{:=} Capacity [ j ] {\p@rbrace}}%
\end{tlatex}
\midcol
\begin{nopcal}
with (j \in Jugs) \* empty jug j
  { injug[j] := 0 }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@with} {\p@lparen} j \.{\in} Jugs {\p@rparen}}%
\@y{%
    \colorbox{gray}{empty jug j}
}%
\@xx{}%
\@x{\@s{8.2} {\p@lbrace} injug [ j ] \.{:=} 0 {\p@rbrace}}%
\end{tlatex}
\end{twocols}
\vspace{.5em}
Remembering the \textbf{with} statement in 
 \lref{seteuclid-body}{the body of algorithm $SetEuclid$} and
how we represented pouring from one jug to another in
\popref{pdiehard}{algorithm $DieHard$}, we see that pouring from a jug
$j$ to a jug $k$ is represented by this PlusCal statement.%

\bigskip
\begin{nopcal}
with (j \in Jugs, k \in Jugs \ {j}) \* pour from jug j to jug k
                { with ( poured = 
                           Min(injug[j] + injug[k], Capacity[k]) - injug[k] )
                    { injug[j] := injug[j] - poured ;
                      injug[k] := injug[k] + poured
                    }
                }
\end{nopcal}
\begin{tlatex}
 \@x{ {\p@with} {\p@lparen} j \.{\in} Jugs ,\, k \.{\in} Jugs
 \.{\,\backslash\,} \{ j \} {\p@rparen}}%
 \@y{%
  \colorbox{gray}{pour from jug j to jug k}
}%
  \marginpar[.5]{\red Warning: this code has an error.}
\@xx{}%
\@x{\@s{72.47} {\p@lbrace} {\p@with} {\p@lparen} poured \.{=}}%
 \@x{\@s{124.75} Min ( injug [ j ] \.{+} injug [ k ] ,\, Capacity [ k ] )
 \.{-} injug [ k ] {\p@rparen}}%
 \@x{\@s{90.25} {\p@lbrace} injug [ j ]\@s{1.16} \.{:=} injug [ j ]\@s{1.16}
 \.{-} poured {\p@semicolon}}%
\@x{\@s{99.83} injug [ k ] \.{:=} injug [ k ] \.{+} poured}%
\@x{\@s{90.25} {\p@rbrace}}%
\@x{\@s{72.47} {\p@rbrace}}%
\end{tlatex}

\bigskip

Add \popref{die-harder-ascii}{the \textsc{ascii} code} of the algorithm
to a comment in the module, save the module, and run the translator.
The translator reports the following error:
\begin{display}
\texttt{Second assignment to same variable inside a `with' statement at line} \ldots 
\end{display}
Clicking on the error message takes you to the second assignment
statement inside the code representing pouring from jug $j$ to jug
$k$.  The message is telling us that we can't have two
assignment statements within the body of a \textbf{with} statement
that assign to the same variable.  To understand why not, you need to
know two additional 
  \ctindex{2}{rules!labeling}{rules-labeling}%
  \ctindex{2}{labels, PlusCal!rules for}{labels-rules}%
rules for labels in a PlusCal algorithm.
\begin{itemize}
\item Two separate assignment statements that assign to the
same variable cannot occur within a single step.

\item The body of a \textbf{with} statement cannot contain a label.
\end{itemize}
Recall that a step consists of execution from one label to the next.
The first rule implies that a label must come between any two
assignment statements to the same variable.  In this case, that means
that the assignment to $injug[k]$ must have a label.  However, the
second rule says that a label cannot go there.  Hence, these two rules
imply that it is impossible for the translator to assign labels to
this algorithm.

The solution to this problem is to replace the semi-colon
(\,\textbf{;}\,) between the assignment statements with
  \ctindex{1}{+2h@\mmath{"|"|} (PlusCal multi-assignment)}{+2h}%
  \tindex{1}{multi-assignment, PlusCal}%
\texttt{|\s{-.1}|} (two \,\texttt{|}\, characters), turning the two
separate assignment statements into this single multi-assignment:
 \[ \begin{noj}
    injug[j] := injug[j] - poured \ \parallel 
                      injug[k] := injug[k] + poured
    \end{noj}
 \]
A multi-assignment statement consists of a sequence of assignments
of the form:
  \[ v_{1} := e_{1} \ \parallel \ \ldots \ \parallel \ v_{n} := e_{n} \]
It is executed by evaluating all the expressions $e_{i}$, and then
simultaneously performing the assignments to all the $v_{i}$.
For example, if $v$ and $w$ are variables, then executing
  \[ v := w \ \parallel \ w := v \]
interchanges the values of the two variables, setting the new value of
$v$ to the old value of $w$ and vice-versa.  A multi-assignment
statement is most often used to assign to multiple ``components'' of a
function in the same step, as in the $DieHarder$ algorithm.

After turning the two assignment statements into one multi-assignment,
the module should look \popref{die-harder}{like this}.  The translator
should now produce no more errors.

\subsection{Checking the Algorithm}

We now use TLC to check algorithm $DieHarder$.  Let's create a model for
which algorithm $DieHarder$ is equivalent to algorithm $DieHard$---that is,
one with two jugs of capacities 3 and 5, and a goal of 4.  Open a new model
and have it set $Goal$ to 4.

We want the model to assign some set of two elements to $Jugs$.  We
could let those elements be two numbers such as 3 and 47, or two
strings such as $"bigJug"$ and $"smallJug"$.  However, when some part
of a spec can have any value, it's usually a good idea to have
the model assign it a 
 \tindex{1}{model value}%
 \ctindex{1}{value!model}{value-model}%
\emph{model value}.  
  \marginpar{To learn
more about model values, see the
\helppage{model/model-values}{\emph{Model Values and Symmetry}}
Toolbox help page.}
A model value is treated by TLC to be a value
about which it knows nothing, except that it is unequal to any other
model value.  We can give a model value any legal identifier name.
Since we're modeling the original Die Hard system, let's make the
elements of $Jugs$ be the model values $big$ and $small$.  On the dialog
for assigning a value to the constant $Jugs$, type 
 $\{big,\,small\}$
into the text field and select the \textsf{Set of model values}
option.  Click on \textsf{Next} and then on \textsf{Finish}.  

We want the model to assign to $Capacity$ a function with domain
$\{big,\,small\}$ such that $Capacity[big]=5$ and $Capacity[small]=3$.
One way of doing this is to assign it the value
 \[ [j \in Jugs |-> \mbox{\textsc{if} } j = big 
       \mbox{ \textsc{then} }  5 \mbox{\ \textsc{else} } 3] 
 \]
However, when creating a model, we can write this function more
conveniently as follows:
\begin{display}
\begin{twocols}[.4]
$(big :> 5) \  @@ \ (small :> 3)$
\midcol 
\verb|(big :> 5) @@ (small :> 3)|
\end{twocols}
\end{display}
(Select the \textsf{Ordinary assignment} option when assigning
the value to $Capacity$.  The assignment of the set of
model values $\{big, small\}$ to $Jugs$ declares $big$ and
$small$ to be model values in this model.)
It should be clear how to use this notation to write any function with
a finite domain.  The operators $:>$ and $@@$, which are defined in
the standard $TLC$ module, are explained in
\rref{math}{\xlink{sec:writing-fcns}}{Section~\xref{sec:writing-fcns}}.
(Operators defined in the $TLC$ module can be used in creating a model,
even if the spec doesn't import that module.)

Add the formula
 \[ \A j \in Jugs : injug[j] # Goal \]
to the model's list of invariants to be checked.  Running TLC should
produce as an error trace a behavior that solves the Die Hard problem
for this selection of jugs.  Try other models.  Start with a model
with two jugs of capacity 3 and 6 gallons, having the goal of
obtaining 4 gallons of water.  TLC will report that the alleged invariant 
actually is an invariant.  For that model, the problem has no solution.

\begin{aquestion}{dieharder-ans}
Under what condition does the generalized Die Hard problem have a
solution?  That is, for what values of the constants is the formula
above not an invariant of algorithm $DieHarder$?
\end{aquestion}

\subsection{The \protect\tlaplus\ Translation}

Let's now consider the \tlaplus\ translation of the algorithm.
Let's start with the \textbf{either} clause, which describes
the action of filling a jug.
\begin{display}
\begin{nopcal}
with (j \in Jugs) \* fill jug j
  { injug[j] := Capacity[j] }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@with} {\p@lparen} j \.{\in} Jugs {\p@rparen}}%
\@y{%
  \colorbox{gray}{fill jug j}
}%
\@xx{}%
\@x{\@s{8.2} {\p@lbrace} injug [ j ] \.{:=} Capacity [ j ] {\p@rbrace}}%
\end{tlatex}
\end{display}
We know that its translation is $\,\E\,j\in Jugs : \Sigma\,$, where
$\Sigma$ is the translation of the assignment statement.  What is the
translation of the assignment statement?  Most people think it should
be
 $\,injug[j]' = Capacity[j]\,$.  Let's see if it is.

In the Toolbox, run TLC on a model that produces an error trace showing a
solution to the Die Hard problem.  The first step in that trace is one
that fills a jug.  Double-click on the row $<$\textsf{Action line
\ldots} between the two states of that first step, which takes you to
the part of the next-state action satisfied by that step.  
Holding down the \textsf{control} control key while double-clicking
takes you to the corresponding place in the PlusCal code, which
is indeed the \textbf{either} clause.  As you can see, the translation
of the assignment statement is not $\,injug[j]' = Capacity[j]\,$.
Why not?

Recall \popref{euclid-body}{the body of algorithm $Euclid$}
and its \popref{euclid-pcal2}{\tlaplus\ translation}.  Observe that
the translation of the assignment statement $y := y-x$ is
not the formula $y'=y-x$.  That formula describes the new value of $y$,
but says nothing about the new value of $x$.  The translation of the
assignment statement is the formula
  \[ \begin{conj}
     y'=y-x \V{.2} x'=x
     \end{conj}
  \]
that also describes the new value of $x$.

Similarly, the translation of the statement $injug[j] := Capacity[j]$
must describe not only the new value of $injug[j]$, but the values of
$injug[k]$ for all $k$ in the domain of $injug$.  The formula
$\,injug[j]' = Capacity[j]\,$ says nothing about the new value of any
expression other than $injug[j]$.  Not only does it say nothing about
the value of $injug[k]'$ for $k#j$, it doesn't even say anything about
the domain of $injug'$.  There are lots of values of $injug'$ that satisfy
this formula---for example, it is satisfied if $injug'$ is the function
 \[ [k \in \{j\} |-> Capacity[j]] \]
whose domain contains only the single element $j$.

The translation of $injug[j] := Capacity[j]$ must be a formula
asserting that $injug'$ is a function that is exactly the same
as the function $injug$, except that $injug'[j]$ equals $Capacity[j]$.
We can write that function as:
 \[ [k \in Jugs |-> \IF{k=j}\THEN Capacity[j]\LSE injug[k]\,]
 \]
The domain of an arbitrary function $f$ can be 
written in \tlaplus\ as
 \ctindex{1}{DOMAIN@\icmd{textsc}{domain}}{DOMAIN}%
\textsc{domain}~$f$.  We can therefore also write the function above as
 \[ [k \in \DOMAIN injug |-> \IF{k=j}\THEN Capacity[j]\LSE injug[k]\,]
 \]
Since assignments to arrays appear frequently in specifications, 
\tlaplus\ provides a more convenient way of writing this function.
As you can see from the translation of algorithm $DieHarder$, it can
be written as%
 \marginpar[-1]{\popref{except-notation}{What do the \textsc{except}
and the \,!\, mean?}}%
%  \marginpar[-1]{\popref{except-notation}{Why this crazy 
% {\rm\textsc{except}} notation?}}%
  \ctindex{1}{except@\icmd{textsc}{except}}{except}%   
 \[ [injug \EXCEPT ![j] = Capacity[j]\,] 
 \]
% have invented for it to be \popref{worse-than-except}{even worse}.  
A further examination of the algorithm's translation shows that
the multi-assignment statement 
 \[ \begin{noj}
    injug[j] := injug[j] - poured \ \parallel 
                      injug[k] := injug[k] + poured
    \end{noj}
 \]
is translated as 
\begin{display}
\begin{notla}
injug' = [injug EXCEPT ![j] = injug[j] - poured,
                       ![k] = injug[k] + poured]
\end{notla}
\begin{tlatex}
 \@x{ injug \.{'} \.{=} [\, injug {\EXCEPT} {\bang} [ j ]\@s{1.16} \.{=} injug [
 j ]\@s{1.16} \.{-} poured ,\,}%
\@x{\@s{106.15}\, {\bang} [ k ] \.{=} injug [ k ] \.{+} poured \,]}%
\end{tlatex}
\end{display}
In general, the expression $[f\EXCEPT ![x]=d,\; ![y]=e]$ is defined to
equal $[\,[f\EXCEPT ![x]=d] \EXCEPT ![y]=e]$. 
The meaning of the further generalization
    $[f\EXCEPT ![x_{1}]=e_{1},\,\ldots\,,\; ![x_{n}]=e_{n}]$ 
should be clear.

\begin{aquestion}{e-distributes}
Explain why changing the body of the \textbf{while} loop 
of algorithm $DieHarder$ to the following produces
an equivalent algorithm.
\begin{display}
\begin{nopcal}
with (j \in Jugs) 
  { either { injug[j] := Capacity[j] }
    or     { injug[j] := 0 } 
    or     with (k \in Jugs \ {j}) 
             { with ( poured = 
                        Min(injug[j] + injug[k], Capacity[k]) - injug[k] )
                 { injug[j] := injug[j] - poured ||
                   injug[k] := injug[k] + poured
                 }
             }
  }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@with} {\p@lparen} j \.{\in} Jugs {\p@rparen}}%
 \@x{\@s{8.2} {\p@lbrace} {\p@either} {\p@lbrace} injug [ j ] \.{:=} Capacity
 [ j ] {\p@rbrace}}%
 \@x{\@s{17.78} {\p@or}\@s{18.84} {\p@lbrace} injug [ j ] \.{:=} 0
 {\p@rbrace}}%
 \@x{\@s{17.78} {\p@or}\@s{18.84} {\p@with} {\p@lparen} k \.{\in} Jugs
 \.{\,\backslash\,} \{ j \} {\p@rparen}}%
\@x{\@s{59.14} {\p@lbrace} {\p@with} {\p@lparen} poured \.{=}}%
 \@x{\@s{111.43} Min ( injug [ j ] \.{+} injug [ k ] ,\, Capacity [ k ] )
 \.{-} injug [ k ] {\p@rparen}}%
 \@x{\@s{76.93} {\p@lbrace} injug [ j ]\@s{1.16} \.{:=} injug [ j ]\@s{1.16}
 \.{-} poured \.{\p@barbar}}%
\@x{\@s{86.51} injug [ k ] \.{:=} injug [ k ] \.{+} poured}%
\@x{\@s{76.93} {\p@rbrace}}%
\@x{\@s{59.14} {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\end{display}
\end{aquestion}
%




%try
\newpage
\tindex{1}{alternation}%
 \vspace{-\baselineskip}%
\section{Alternation} 
%\section{The One-Bit Clock Revisited}

\subsection{The Problem}

We now begin the subject that concerns most of the \emph{Principles}
track of this hyperbook: multiprocess algorithms and systems.  We
start with \emph{alternation}, which is the simplest form of
multiprocess synchronization.  In alternation synchronization, two
processes each have an operation to perform, and they must execute
those operations alternately.

Let's call the processes the 
  \tindex{1}{producer}%
\emph{producer} and the 
  \tindex{1}{consumer}%
\emph{consumer},
and let their operations be called $put$ and $get$, respectively.  The
two processes must cooperate to perform the sequence of operations:
 \[ put \ -> \ get \ -> \ put \ -> \ get \ -> \ldots \]
Think of the $put$ operation as putting an object into a box, and the
$get$ operation as taking the object out of the box and doing
something with it.

To express the problem in our \lref{main:standard-model}{Standard
Model}, we let the variable $box$ represent the state manipulated
by the operations.  For simplicity, we represent the $put$ and $get$
operations by these two PlusCal statements
\begin{display}
\begin{twocols}[.3]
$box := Put(box)$
\midcol
$box := Get(box)$
\end{twocols}
\end{display}

\begin{aquestion}{non-deterministic-box}
This representation makes the $put$ and $get$ operations deterministic.
Given the initial value $b_{0}$ of $box$ in a behavior, the sequence
of values of $box$ in the behavior must be
 \[ [box=b_{0}] \ -> \ [box=Put(b_{0})]  \ -> \ 
    [box=Get(Put(b_{0}))]  \ -> \ [box=Put(Get(Put(b_{0})))]  
    \ -> \ \ldots \s{-20}
 \]
How can we modify the representation to allow the $put$ and $get$
operations to be nondeterministic?
\end{aquestion}
%
We can declare $Put$ and $Get$ to be parameters of a specification
with the statement
\begin{display}
$\CONSTANTS Put(\_), \ Get(\_)$
\end{display}
However, each TLC model that we use would then have to assign
particular operators to $Put$ and $Get$.  For convenience, we
define specific operators $Put$ and $Get$.  

I could probably think of hundreds of sensible ways to define $Put$
and $Get$.  For reasons that may (or may not) become clearer in a
later section, I like to define them so the value of $box$ is a tuple
that initially equals the 0-tuple $<<\,>>$.  We define $Put$ to be the
operator that appends the value $"widget"$ to a tuple, so
 \[ Put(<<e_{1}, \ldots\,, e_{n}>>) = 
     <<e_{1}, \ldots\,, e_{n}, "widget">>
 \]
We define $Get$ to be the operator the removes the first element
of a tuple, so
 \[ Get(<<e_{1}, \ldots\,, e_{n}>>) = 
     <<e_{2}, \ldots\,, e_{n}>>
 \]
Thus, a behavior should have the following sequence of values of
$box$:
 \[ [box = <<\,>>] \ -> \ 
    [box = <<"widget">>] \ -> \ 
    [box = <<\,>>] \ -> \ 
    [box = <<"widget">>] \ -> \ \ldots
 \]
These definitions make it easy to have TLC check that an algorithm
does execute the two operations alternately.  Executing two successive
$put$ operations sets $box$ to a tuple containing two or more
elements.  If that doesn't happen, then executing two successive $get$
operations causes the second one to try to remove the first element of
a 0-tuple, which will produce a TLC error.  Having TLC check the
invariant that the tuple $box$ has at most one element will therefore
produce an error if the two operations are not executed alternately.

In \tlaplus, tuples are the same as finite sequences, an $n$-tuple
being a sequence of length~$n$.  We define $Put$ and $Get$ using the
operators $Append$ and $Tail$ defined in the
  \ctindex{1}{Sequences module@\mmath{Sequences} module}{sequences-module}%
\rref{math}{math:sequences-module}{standard $Sequences$ module}.  
The definitions are:%
 \target{main:put-get}
\begin{display} 
\begin{twocols}[.4]
\begin{notla}
Put(s) == Append(s, "widget")
Get(s) == Tail(s)
\end{notla}
\begin{tlatex}
\@x{ Put ( s )\@s{0.18} \.{\defeq} Append ( s ,\,\@w{widget} )\vs{.2}}%
\@x{ Get ( s ) \.{\defeq} Tail ( s )}%
\end{tlatex}
\midcol
\verb|Put(s) == Append(s, "widget")|\V{.2}
\verb|Get(s) == Tail(s)|
\verb||
\end{twocols}
\end{display}
The $Sequences$ module also defines the operator $Len$ so that
$Len(s)$ is the length of a sequence~$s$.  We can have TLC check that
an algorithm implements alternation by checking the invariance of
$Len(box) \leq 1$.

\subsection{The One-Bit Clock Revisited} \xlabel{one-bit-revisited}

The simplest way to implement alternation is with a one-bit clock that
starts at~0.  The $put$ operation is performed when the clock changes
from 0 to~1, and the $get$ operation is performed when it changes from
1 to~0.  So, let's return to our first example: \lref{sec:one-bit}{the
one-bit clock}.
%
We again represent the value of the clock with a variable $b$.  
However, this time we represent the clock as a two-process algorithm: a
\emph{Tick} process that changes the value of $b$ from~0 to~1, and a
\emph{Tock} process that changes it from~1 to~0.  The algorithm begins
as usual, with the declaration of variable $b$ setting its initial value
to~0.
\begin{display}
\begin{nopcal}
--algorithm TickTock  {
    variable b = 0 ;
\end{nopcal}
\begin{tlatex}
\@x{ {\p@mmalgorithm} TickTock\@s{4.1} {\p@lbrace}}%
\@x{\@s{16.4} {\p@variable} b \.{=} 0 {\p@semicolon}}%
\end{tlatex}
\end{display}
In PlusCal, a 
 \ctindex{1}{process!PlusCal}{process-pluscal}%
process has both a name and an 
  \tindex{1}{process identifier}%
  \ctindex{1}{identifier, process}{id-process}%
\emph{identifier} (\emph{id} for short), which can be any value.  We
let the process named $Tick$ have id~0 and the one named $Tock$ have
id~1.  Here is \popref{tick-tock-procs}{the code for the two
processes}.  The order of the two \textbf{process} declarations makes
no difference.  By default, the translator requires us to provide
labels for a multiprocess algorithm.  Since a step consists of
execution from one label to the next, the single label in each process
makes an entire execution of its \textbf{while} loop's body a single
step.

 Intuitively, the statement
  \ctindex{1}{await (PlusCal statement)@\icmd{textbf}{await} (PlusCal statement)}{await-pcal}%
\textbf{await}~$b=0$ causes process $Tick$ to wait until $b$ equals~0
before it can execute the rest of the step.  However, that intuitive
explanation shouldn't be taken too seriously.  The real meaning of an
\textbf{await} statement is revealed by its
\tlaplus\ translation. 


Open a new specification named $TickTock$ and insert into it
\popref{ticktock-ascii}{the \textsc{ascii} version} of the algorithm.
Run the translator, and then create a model and run TLC on it.  There
should be no error. TLC reports as expected that there are two
reachable states.  Now examine the initial predicate $Init$ and the
next-state action $Next$ generated by the \tlaplus\ translation.  The
predicate $Init$ obviously is defined to equal $b=0$ (with a
superfluous $\land$).  The action $Next$ is the disjunction of the
two subactions $Tick$ and $Tock$.  From their definitions, we see that
$Next$ equals
 \[\begin{disj}
    \begin{conj}
    b=0 \\ b' = 1
    \end{conj} \\
    \begin{conj}
    b=1 \\ b' = 0
    \end{conj}
   \end{disj}
  \]
This is the \lref{main:next1}{action $Next1$} that we defined above in
our first specification of the one-bit clock.  

The Toolbox's 
    \ctindex{1}{Goto PCal Source (Toolbox command)@\icmd{textsf}{Goto PCal Source} (Toolbox command)}{gotopcalsource}%
\textsf{Goto PCal Source} command allows you to find the
PlusCal code corresponding to parts of the \tlaplus\ translation.
Select a region of the translation and run the command either from the
module editor's right-click menu or by typing \textsf{F10}.  The
command jumps to and highlights the PlusCal source.  If you try that
on the translation, you will discover that the statement
\textbf{await}~$P$ simply adds the conjunct $P$ to the appropriate
part of the next-state action.

\begin{aquestion}{one-proc-ticktock} \label{question:ticktock}
Write a uniprocess PlusCal algorithm whose translation defines exactly
the same definitions of $Init$ and $Next$ as that of algorithm $TickTock$.
\end{aquestion}
%
Is the one-bit clock a uniprocess system or a multiprocess system?
It's neither.  The number of processes is a property of the syntactic
representation of a system in PlusCal or in some programming language.
It is not a property of the system itself.  The true meaning of (a
model of) a system lies in the mathematics: the initial predicate and
next-state action (plus perhaps a fairness formula).  As we saw in our
\lref{sec:one-bit}{original discussion of the one-bit clock}, there
are many ways to write the initial predicate and next-state action to
produce the same behavior specification---that is, to produce equivalent
temporal formulas $Init /\ [][Next]_{vars}$.  As shown by
Question~\ref{question:ticktock}, different PlusCal code can even
produce syntactically identical behavior specifications.


\subsection{Specifying Alternation: Safety} 

We now specify alternation synchronization by adding to the one-bit
clock specification the $put$ and $get$ operations, expressed using
the $Put$ and $Get$ operators \lref{main:put-get}{defined above}.  The
specification is in \popref{alternation}{module $Alternation$}.

The module first imports the standard $Integers$ and $Sequences$
modules and defines $Put$ and $Get$.  Then comes the behavioral
specification, written as a PlusCal algorithm called $Alternate$.  The
algorithm declares the variable $b$ of the one-bit clock, with
initial value~0, and the variable $box$ used by the $put$ and $get$
operations, with initial value equal to the empty sequence.

The producer and consumer processes are specified by 
    \ctindex{1}{process (PlusCal declaration)@\icmd{textbf}{process} (PlusCal declaration)}{process}%
\textbf{process}
declarations, which are the same as the the two clock processes except
with the added assignments to $box$ that represent the $put$ and $get$
operations.  The labels have also been changed.  Create the
$Alternation$ specification in the Toolbox, using
\popref{alternation-ascii}{this \textsc{ascii} version} of the
module's body.

It should be clear that the only behaviors of this algorithm are
the infinite behavior%
  \target{alt-behav}%
 \[
  \altstate{0}{<<\,>>} \;->\; \altstate{1}{<<"widget">>} \;->\; 
  \altstate{0}{<<\,>>} \;->\; \altstate{1}{<<"widget">>} \;->\; \cdots
 \] 
and any finite prefix of it.  (We haven't specified any fairness
requirement yet, so execution can stop at any point.)  Hence, except for
fairness, the sequence of values of $box$ specifies the proper
alternation of the $put$ and $get$ operations.  Let's use TLC to check
that this is the case, and that we haven't made a mistake.  Create a
new model, add to it the invariant $Len(box) \leq 1$, and run TLC on
it.  TLC reports that the invariant is satisfied, and that there are
just two reachable states, as expected.

There are two problems with this specification.  I've mentioned the
first: that we haven't specified any fairness requirements.  The spec
therefore describes only the safety properties of alternation, not any
liveness properties.  Liveness is discussed below.  

The second problem is that I informally described alternation in terms
of $put$ and $get$ operations, which are represented in terms of the
single variable $box$.  However, algorithm $Alternation$ also
specifies the values of the variable $b$.  The variable $b$ serves
only to ``control'' the possible values of the variable $box$.  We
don't care about its value.  It's possible to write a philosophically
correct specification that hides
  \marginpar{\popref{variable-hiding}{How to hide variables in TLA.}}%
the variable $b$, but we won't bother with philosophy.  Instead, we
take a practical approach that depends on what it means to implement
the specification.  This approach is discussed later.


\subsection{Specifying Alternation: Liveness} \xlabel{sec:alt-liveness}

We saw in 
 \lref{\xlink{main:liveness}}{Section~\xref{main:liveness}}
that the initial predicate $Init$ and next-state action $Next$ assert
the following safety properties of a behavior $s_{1}-> s_{2} ->
\cdots$\,.
\begin{enumerate}
\item $Init$ is true in $s_{1}$.
(Remember that a state is an assignment of values to variables.)

\item Each step $s_{i}->s_{i+1}$ of the behavior is a $Next$ step.
\end{enumerate}
Recall that condition~2 means that formula $Next$ is true when each
unprimed variables is replaced by its values in state $s_{i}$ and each
primed variable is replaced by the variable's value in state
$s_{i+1}$.

Beginning the algorithm with \,\texttt{-\mbox{}-fair algorithm}\,
adds the fairness requirement $\WF_{var}(Next)$, which asserts:
\begin{enumerate}
\item[3.] The behavior does not end in a state $s_{n}$ if there exists
a state $s_{n+1}$ such that the sequence $s_{1}->\ldots->s_{n+1}$ also
satisfies condition~2.
\end{enumerate}
We say that an action $A$ is 
  \tindex{1}{enabled}%
\emph{enabled} in a state $s$ iff there exists a state $t$ such that
$s->t$ is an $A$ step.  Assuming the truth of condition~2, we can
restate condition~3 as:
\begin{enumerate}
\item[3a.] The behavior does not end in a state $s_{n}$ in which
$Next$ is enabled.
\end{enumerate}
For algorithm $Alternate$, the next-state action $Next$ is enabled in
every reachable state, so weak fairness of $Next$ implies that 
it has no finite behaviors.  It must execute forever.

This same liveness condition can be expressed as weak fairness of each
of the processes.  For an arbitrary action $A$, the 
  \tindex{1}{fairness}%
  \ctindex{1}{fairness!weak}{fairness-weak}%
weak fairness requirement 
  \ctindex{1}{WF@\icmd{WF}}{WF}%
$\WF_{var}(A)$ is defined to assert the following
two conditions for the behavior $s_{1}-> s_{2} -> \cdots$\,.
\begin{enumerate}
\item[3a.] The behavior does not end in a state $s_{n}$ in which
$A$ is enabled.

\item[3b.] If the behavior is infinite, then there is no $n$ such
that the infinite behavior $s_{n}->s_{n+1}-> \cdots$ has no $A$ steps
but $A$ is enabled in all of its states.
\end{enumerate}
If $A$ is the next-state action $Next$, then condition~3b is implied
by condition~2, which asserts that every step of the behavior is a
  \marginpop{strong-fairness2}{What is strong fairness?}%
$Next$ step.

Preceding the keyword 
 \ctindex{1}{fair process@\icmd{fair} \icmd{process}}{fair-pcal}%
\textbf{process} by \textbf{fair} causes the
translator to conjoin weak fairness of that process's next-state
action to the specification $Spec$.  For example, change the
$Producer$ process's declaration in algorithm $Alternate$ so it begins:
\begin{display}
\textbf{fair process (} $Producer = 0$ \textbf{)}
\end{display}
and run the translator.  This produces the definition
\begin{display}
\begin{notla}
Spec == /\ Init /\ [][Next]_vars
        /\ WF_vars(Producer)
\end{notla}
\begin{tlatex}
\@x{ Spec \.{\defeq} \.{\land} Init \.{\land} {\Box} [ Next ]_{ vars}}%
\@x{\@s{38.36} \.{\land} {\WF}_{ vars} ( Producer )}%
\end{tlatex}
\end{display}
The action $Producer$ is the next-state action of process $Producer$,
which describes an execution of the body of the process's
\textbf{while} loop.  That action is enabled in a state $s$ iff $s$
assigns the value~0 to $b$.  Conditions~1 and~2 imply that every
second step of a behavior is a $Producer$ step.  Hence, with this
definition, $Spec$ asserts that a behavior cannot stop in a state with
$b=0$.  Similarly, fairness of the $Consumer$ process implies that a
behavior cannot stop in a state with $b=1$.  Fairness of both
processes therefore implies that a behavior must be infinite, so it is
equivalent to fairness of the next-state action.  The equivalence of
weak fairness of all processes and weak fairness of the next-state
action for a system specification indicates a special class of system
in which stopping one process causes the entire system to stop.


\medskip

What liveness requirement do we want for alternation synchronization?
There is no single answer.  In some applications, the producer and
consumer represent two components of a system that we want to model as
running forever.  In that case, we want fairness of both
processes---or equivalently, fairness of the entire algorithm's
next-state action.  In other applications, the producer represents a
client whose $put$ operations are requests, and the consumer
represents a server whose $get$ operations are responses.  In that
case, we usually require the server to respond to each request, but
don't require the client to keep sending requests.  The requirement
that the server responds to each request is expressed by fairness of
the $Consumer$ process, since that implies that a $Consumer$ step must
occur after every $Producer$ step.

Let's ignore fairness for now.  Eliminate all fairness from the
algorithm and run the translator.


\subsection{The Two-Phase Handshake Protocol}

We now consider a common hardware signaling protocol for implementing
alternation synchronization called the
  \tindex{1}{two-phase handshake}%
  \tindex{1}{handshake}%
\emph{two-phase handshake}.  
As illustrated here,
\begin{display}
\setlength{\unitlength}{1.3pt}
\begin{picture}(40,34)(0,0)
\put(0, 0){\framebox(40,30){$Producer$}}
\put(90, 0){\framebox(40,30){$Consumer$}}
\put(40,23){\vector(1,0){50}}
\put(90,7){\vector(-1,0){50}}
\put(65, 25){\makebox(0,0)[b]{$p$}}
\put(65, 5){\makebox(0,0)[t]{$c$}}
\end{picture}
\end{display}
the processes communicate using two one-bit ``wires'' $p$ and $c$,
where $p$ is set by the producer and read by the consumer, and
$c$ is set by the consumer and read by the producer.



% \marginpar{
% \setlength{\unitlength}{.82pt}
% \begin{picture}(40,30)(0,20)
% \put(0, 0){\framebox(40,30){$Prod$}}
% \put(90, 0){\framebox(40,30){$Cons$}}
% \put(40,23){\vector(1,0){50}}
% \put(90,7){\vector(-1,0){50}}
% \put(65, 25){\makebox(0,0)[b]{$p$}}
% \put(65, 5){\makebox(0,0)[t]{$c$}}
% \end{picture}}

We describe the algorithm in terms of the operator 
  \marginpar{$\oplus$\, is typed \,\texttt{(+)}\, or
                \,\texttt{$\backslash$oplus}\,.}
$\oplus$, which is defined  by:
 \[ a \oplus b == (a + b) \;\%\; 2\]
We are interested in applying $\oplus$ only to elements in
\marginpar{$
   \begin{noj2}
     0 \oplus 0 = 0 \s{1} & 0 \oplus 1 = 1 \V{.2}
     1 \oplus 0 = 1 \s{1} & 1 \oplus 1 = 0 
     \end{noj2}
  $}
the set $\{0,\,1\}$, for which its table of values is shown in the
margin.  When restricted to the set $\{0,\,1\}$, the
operator $\oplus$ is called
  \tindex{1}{exclusive or}%
\emph{exclusive or}.  Observe that if $a$ equals 0 or 1, then
$a\oplus1$ is the ``complement'' of $a$.

We represent the two-phase handshake protocol as algorithm $Handshake$
in \popref{handshake}{the $Handshake$ module}.  Compare the bodies
of the processes' \textbf{while} loops in algorithm $Handshake$
with those in \popref{alternation}{algorithm $Alternation$}:
\begin{display}
$\begin{noj3}
  & \underline{Handshake} & \underline{Alternation} \V{.3}
Producer\!\!: & \begin{noj}
           \textbf{await} \ p=c \,\textbf{;} \\
           box := Put(box) \,\textbf{;} \\
           p := p \oplus 1
           \end{noj} \s{2}
  &        \begin{noj}
           \textbf{await} \ b=0 \,\textbf{;} \\
           box := Put(box) \,\textbf{;} \\
           b := 1 \vs{.5}
           \end{noj} \\
Consumer\!\!:\s{.2} & \begin{noj}
           \textbf{await} \ p#c \,\textbf{;} \\
           box := Get(box) \,\textbf{;} \\
           c := c \oplus 1
           \end{noj}
  &        \begin{noj}
           \textbf{await} \ b=1 \,\textbf{;} \\
           box := Get(box) \,\textbf{;} \\
           b := 0 
           \end{noj} 
 \end{noj3}$
\end{display}
\begin{aquestion}{handshake-question} 
 \targetlabel{hs-behavior} 
Write down the single infinite behavior of
algorithm $Handshake$.  In each of the states of that behavior,
write down the value of $p\oplus c$.
\end{aquestion}
%
Create specification $Handshake$ in the Toolbox, using this
\popref{handshake-ascii}{\textsc{ascii} version of its body}, and run
the PlusCal translator on it.  Have TLC check that the formula
\mbox{$Len(box)\leq1$} is an invariant of the algorithm, as it should
be for an implementation of alternation synchronization with these
definitions of $Put$ and $Get$.


\subsection{Refinement} \xlabel{sec:refinement}

Compare the answer to 
  \lref{\xlink{hs-behavior}}{Question~\xref{hs-behavior}},
with the 
  \lref{alt-behav}{behavior of algorithm $Alternation$}.
You will see that the values of $p\oplus c$ in the behavior of
algorithm $Handshake$ equal the values of $b$ in the corresponding
behavior of algorithm $Alternation$.  We can view algorithm
$Handshake$ as implementing the variable $b$ of algorithm
$Alternation$ with the expression $p\oplus c$.  It also implements the
variable $box$ of algorithm $Alternation$ with the expression $box$.
(Algorithm $Handshake$ is thus implementing the variable $box$ of the
$Alternation$ algorithm with its own variable $box$.)

Let \ov{b} and \ov{box} be the expressions (containing the variables
of $Handshake$) that implement the variables of algorithm $Alternation$:
  \[ \ov{b} == p \oplus c \s{2} \ov{box} == box
  \]
Let's write a behavior of algorithm $Handshake$ showing the values of
\ov{b} and \ov{box} in all the states.%
\begin{display}
$\hdskstate{0}{0}{<<\,>>}{0} \;->\; \hdskstate{1}{0}{<<"widget">>}{1} \;->\; 
  \hdskstate{1}{1}{<<\,>>}{0} \;->\;$\V{1}
$\s{2}\hdskstate{0}{1}{<<"widget">>}{1} \;->\; 
   \hdskstate{0}{0}{<<\,>>}{0} \;->\; 
            \cdots$
\end{display}
If we delete the values of the variables $p$, $c$, and $box$ and erase
the overbars from \ov{b} and \ov{box}, we get the sequence
  \[
  \begin{noj}
  \altstate{0}{<<\,>>} \;->\; \altstate{1}{<<"widget">>} \;->\; 
  \altstate{0}{<<\,>>} \;->\; \altstate{1}{<<"widget">>} \;->\;\V{1.5} 
   \s{2}\altstate{0}{<<\,>>} \;->\; \cdots
  \end{noj}
 \] 
which is a behavior of algorithm $Alternation$.  The definitions of
$\ov{b}$ and $\ov{box}$ are called a
  \tindex{1}{refinement mapping}%
  \ctindex{1}{mapping!refinement}{mapping-refinement}%
  \target{refinement-mapping-def}%
\emph{refinement mapping} from $Handshake$ to $Alternation$, 
and we say that algorithm $Handshake$ 
   \tindex{1}{implements under refinement mapping}%
implements algorithm $Alternation$ under this refinement mapping.

\medskip 
I now generalize what we've done for the particular
algorithms $Handshake$ and $Alternation$ to two arbitrary
specifications $H$ and $A$.  To do this, I need to distinguish between
the
  \lref{two-meanings-of-spec}{two meanings of the term \emph{specification}}.
In the following discussion, a \emph{specification} $S$ is a \tlaplus\
module, together with any modules that it imports.  This module must
define a \emph{behavior specification}, which is a temporal formula
(usually named $Spec$).  A state of $S$ is an assignment of values to
the variables declared in $S$; it is not necessarily a reachable state
of $S$'s behavior specification.  A behavior of $S$ is any sequence of
states of $S$; it does not necessarily satisfy $S$'s behavior
specification.

A \emph{refinement mapping} from a specification $H$ to a
specification $A$ is an assignment of an expression $\ov{v}$ to each
variable $v$ of $A$, where \ov{v} is defined in terms of the variables
of $H$.  This refinement mapping defines, for each state $s$ of $H$,
the state \ov{s} of $A$ that assigns to each variable $v$ of $A$ the
value of \ov{v} in state $s$.  If $\sigma$ is the behavior
  $s_{1}-> s_{2}-> \cdots $
of $H$, we define the behavior \ov{\sigma} of $A$ to be
    $\ov{s_{1}} -> \ov{s_{2}} -> \cdots$\,.
We say that $H$ \emph{implements} $A$ under this refinement mapping
iff, for each behavior $\sigma$ satisfying the behavior specification
of $H$, the behavior \ov{\sigma} satisfies the behavior specification
of $A$.

Having precisely defined the meaning of implementation under a
refinement mapping, I will resume the informal use of the term
\emph{specification}.  If you are confused by any informal statement
that I write, translate it into a more rigorous one that clearly
distinguishes between modules and their behavior specifications.

\medskip

We have seen that $Handshake$ implements $Alternation$ under the
refinement mapping defined above.  Since the refinement mapping
defines \ov{box} to equal $box$, this implies that for any behavior
$\sigma$ allowed by algorithm $Handshake$, the behavior \ov{\sigma}
allowed by algorithm $Alternation$ has the same sequence of values of
$box$.  Since correctness of an alternation synchronization depends
only of the values assumed by $box$, this implies that $Handshake$
implements alternation synchronization.  In general, we define an
algorithm to implement alternation synchronization (for these
particular definitions of $Put$ and $Get$) to mean that it implements
algorithm $Alternation$ under a refinement mapping that defines
\ov{box} to equal $box$.

As the following problem shows, whether or not it is interesting that
one specification implements another under a refinement mapping
depends very much on the refinement mapping.  In general, we can
define correctness as implementation under a refinement mapping
satisfying some condition.  However, we are often content to show that
a system or algorithm satisfies certain desired properties.

\begin{aquestion}{handshake-question2}
Let $Count$ be a specification with a single variable $n$ whose
behavior specification allows the single infinite behavior
 \[ [n = 1] \;->\; [n = 2] \;->\; [n = 3] \;->\; [n = 4] \;->\; \cdots
 \]
Show that if $A$ is any specification whose behavior specification allows
an infinite behavior, then $Count$ implements $A$ under some refinement
mapping.
\end{aquestion}

\vspace{-.5em}

\begin{aquestion}{handshake-question3}
Show how to express the property that a formula $I$ is an invariant of
a specification $S$ as the property that $S$ implements a specification
under a refinement mapping.
\end{aquestion}

We can use TLC to check if one specification implements another under
a refinement mapping.  In module $Handshake$, after the algorithm's
translation, add this statement which we say 
  \tindex{1}{instantiates}%
\emph{instantiates} module 
$Alternation$.%
 \ctindex{1}{instance@\icmd{textsc}{instance}}{instance}%
\begin{display}
$A == \INSTANCE Alternation \WITH b <- p \oplus c,\  box <- box$
\ascii{instance-alt-ascii}
\end{display}
We'll see later what this statement means.  For now, just observe that
the \textsc{with} part describes the refinement mapping.  If a
variable $v$ of the instantiated module does not appear in a
\textsc{with} clause, then the clause $v <- v$ is assumed.  Thus, the
clause $box <- box$ can be eliminated from this statement.  We could
eliminate the entire \textsc{with} part by preceding the statement with
the definition
 \[ b == p \oplus c\]
We can use any identifier in place of $A$ (as long as it's not already
defined or declared).

\begin{sloppypar}
To have TLC 
  \ctindex{1}{property!checking with TLC}{property-TLC}%
check that algorithm $Handshake$ implements algorithm
$Alternation$ under this refinment mapping, open a model and, in the
\textsf{Properties} subsection of the \textsf{What to check?} section
of the \textsf{Model Overview} page, add the property $A!Spec$.  This
tells TLC to check that the $Handshake$ specification with its
behavior specification $Spec$ (indicated by the model), implements the
$Alternation$ specification with its behavior specification $Spec$,
under the refinment mapping described by the \textsc{instance}
statement.  Run the model.  TLC should find no error, confirming that
$Handshake$ implements $Alternation$ under the refinement mapping.
\end{sloppypar}

Now, modify the algorithm of module $Alternation$ by making the
$Consumer$ process a \textbf{fair~process}.  Run the translator on
that module and run TLC again on the same model (of specification
$Handshake$).  This time, TLC should report:
\begin{display}
\tt Temporal properties were violated.
\end{display}
and should produce an error trace that halts after the second state.
(Again, stopping is indicated by the mysterious 
 $<$\textsf{Stuttering}$>$, which will soon be explained.)
This behavior corresponds under the refinement mapping to a behavior
that stops after the producer takes a step, which is not allowed by
the fairness requirement for process $Consumer$ in algorithm
$Alternation$.

Add fairness to process $Consumer$ of algorithm $Handshake$ and run
the translator.  TLC should now confirm that $Handshake$ (with
fairness of the consumer) implements $Alternation$ (with fairness of
the consumer) under the refinement mapping.  Fairness of the
$Consumer$ process of algorithm $Handshake$ rules out the behavior of
the algorithm found by TLC showing that $Handshake$ (without consumer
fairness) did not implement $Alternation$ (with consumer fairness)
under the refinment mapping.

\bigskip

\noindent
\popref{derivation}{\textbf{Detour } \textsf{Deriving the handshake
  protocol from the alternation specification.}}


\target{stuttering}
\subsection{Refinement and Stuttering} 

\subsubsection{Adding Steps} \xlabel{ref-and-stutter}

In a more accurate model of the two-phase handshake protocol, execution
of one iteration of a process's \textbf{while} loop would consist of
multiple separate steps.  Let's split each iteration into two steps: the
first executing the \textbf{await} statement, the second executing the
two assignment statements.  We do this by adding a label to each process
as follows:
\begin{display}
\begin{twocols}[.35]
\begin{nopcal}
p1: while (TRUE)
      {     await p = c ;
        p2: box := Put(box) ;
            p := p (+) 1
      }
\end{nopcal}
\begin{tlatex}
\@x{ p1\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen} {\TRUE} {\p@rparen}}%
\@x{\@s{25.22} {\p@lbrace}\@s{16.4} {\p@await} p \.{=} c {\p@semicolon}}%
 \@x{\@s{33.42} p2\@s{.5}\textrm{:}\@s{3}\@s{0.76} box \.{:=} Put ( box )
 {\p@semicolon}}%
\@x{\@s{51.20} p \.{:=} p \.{\oplus} 1}%
\@x{\@s{25.22} {\p@rbrace}}%
\end{tlatex}
\midcol
\begin{nopcal}
c1: while (TRUE)
      {     await p # c ;
        c2: box := Get(box) ;
            c := c (+) 1
      }
\end{nopcal}
\begin{tlatex}
\@x{ c1\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen} {\TRUE} {\p@rparen}}%
\@x{\@s{24.64} {\p@lbrace}\@s{16.4} {\p@await} p \.{\neq} c {\p@semicolon}}%
 \@x{\@s{32.84} c2\@s{.5}\textrm{:}\@s{3}\@s{1.34} box \.{:=} Get ( box )
 {\p@semicolon}}%
\@x{\@s{50.62} c \.{:=} c \.{\oplus} 1}%
\@x{\@s{24.64} {\p@rbrace}}%
\end{tlatex}
\end{twocols}
\end{display}
Run the translator and look at the translation.  The first thing we notice
is that the translation has added a variable $pc$ to represent the control
state.  It defines $ProcSet$ to equal $\{0\} \cup \{1\}$, which equals
the set $\{0,\,1\}$ of process ids.  The initial predicate $Init$
therefore states that the initial value of $pc$ is%
\begin{display}
\begin{notla}
[self \in {0, 1} |-> CASE self = 0 -> "p1"
                       [] self = 1 -> "c1"]
\end{notla}
\begin{tlatex}
 \@x{ [ self \.{\in} \{ 0 ,\, 1 \} \.{\mapsto} {\CASE} self \.{=} 0
 \.{\rightarrow}\@w{p1}}%
\@x{\@s{88.40} {\Box}\@s{4.30} self \.{=} 1 \.{\rightarrow}\@w{c1} ]}%
\end{tlatex}
\end{display}
You can probably guess that $Init$ asserts that $pc$ equals a function
with domain $\{0,\,1\}$ such that $pc[0]$ equals $"p1"$ and $pc[1]$
equals $"c1"$, from which you can figure out the meaning of
\rref{math}{case-expr}{the \textsc{case} construct}.

Let's have TLC display the beginning of a behavior of this algorithm.
Add the formula $pc[1]="c1"$ as an invariant to your model and run
TLC\@.  It should produce an error trace containing four states, the
last state violating this ``invariant'' because it has $pc[1]$ equal
to $"c2"$.  You should be able to figure out \rref{math}{at-at}{how
TLC uses the operators $@\!@$ and $:>$ to write functions}.  (Clicking
on the \,{\setlength{\fboxsep}{.5pt}\footnotesize\fbox{+}}\, next to
$pc$ in one of the states may help.)  We can continue this behavior
to:
\begin{widedisplay}
%\begin{display}
$\hdskkstate{0}{0}{<<\,>>}{"p1"}{"c1"} \;->\; 
 \hdskkstate{0}{0}{<<\,>>}{"p2"}{"c1"} \;->\; 
\hdskkstate{1}{0}{<<"widget">>}{"p1"}{"c1"}  \;->\;$ \V{1}
\s{2}$\hdskkstate{1}{0}{<<"widget">>}{"p1"}{"c2"}  \;->\; 
  \hdskkstate{1}{1}{<<\,>>}{"p1"}{"c1"} \;->\;
  \hdskkstate{1}{1}{<<\,>>}{"p2"}{"c1"} \;->\;$\V{1}
\s{2}$\hdskkstate{0}{1}{<<"widget">>}{"p1"}{"c1"} \;->\; 
%   \hdskkstate{0}{0}{<<\,>>}{0} \;->\; 
            \cdots$
%\end{display}
\end{widedisplay}
%
Letting this be the behavior $s_{1}-> s_{2} -> \cdots$, let's compute
the corresponding behavior $\ov{s_{1}} -> \ov{s_{2}} -> \cdots$ of
module $Alternate$ defined by the refinement mapping 
  $\ov{b}\deq p\oplus c$,
  \marginpar{\popref{trace-explorer}{Using TLC's Trace Explorer to
                                    compute the values of \ov{b}.}}
  $\ov{box}\deq box$.
Using the same procedure as before (adding the values of \ov{b} and
\ov{box} to each state, deleting the variables $p$, $c$, $box$, and
$pc$ of $Handshake$, and erasing the overbars from \ov{b} and
\ov{box}), we get this behavior:
\begin{widedisplay}
  $
  \altstate{0}{<<\,>>} \;->\; \altstate{0}{<<\,>>} \;->\; 
   \altstate{1}{<<"widget">>} \;->\; \altstate{1}{<<"widget">>} \;->\;$ \V{.75} 
  \s{2}$\altstate{0}{<<\,>>} \;->\; \altstate{0}{<<\,>>} \;->\; 
  \altstate{1}{<<"widget">>} \;->\;  \cdots
 $
\end{widedisplay}
This behavior has stuttering steps---steps that repeat the same
state---that were not in the behaviors of algorithm $Alternation$ that
we have computed.  So, it looks like this version of algorithm
$Handshake$, with the additional labels, doesn't implement
$Alternation$ under the refinement mapping.  Let's have TLC verify
that it doesn't.  Run TLC on the same model as before, except with
$pc[1]="c1"$ removed from the list of invariants.  TLC reports no
error! What's going on?


\subsubsection{Temporal Logic and Stuttering} \xlabel{fairness-revisited}

\begin{sloppypar}
To understand why this version of algorithm $Handshake$ still
implements $Alternation$ under the refinement mapping, we must examine
the temporal formulas $Spec$ that are the behavior specifications of
our algorithms.  Ignoring fairness, these specifications $Spec$ have
the form $Init /\ [][Next]_{vars}$.  A non-temporal formula like
$Init$ or $Next$ is an assertion about a step.  Formula $Next$ is true
of a step $s->t$ iff it is true when we substitute for each unprimed
variable $v$ the value of $v$ in state $s$, and for each primed
variable $v'$ the value of $v$ in state $t$.  Since $Init$ has no
primed variables, whether or not it is true on a step $s -> t$ 
depends only on $s$, not on $t$.  We can therefore think of $Init$
being true or false for a state.
\end{sloppypar}

We consider a formula without any temporal operators to be the
temporal formula that is true of a behavior iff it is true of the
first step of that behavior.  Since the truth of $Init$ in a step
depends only on the step's first state, $Init$ is the temporal formula
that is true of a behavior iff it is true in the behavior's first
state.

For a formula $F$ with no temporal operators, $[]F$ is the temporal
formula that is true of a behavior iff it is true of \emph{every} step
of the behavior.  Thus, the formula $Init /\ [][Next]_{vars}$ is true
of a behavior iff $Init$ is true of the first state of the behavior
and $[Next]_{vars}$ is true of every step in the behavior.  For any
 \ctindex{1}{+4m@\mmath{[A]_{v}} (action operator)}{+4m}%
action formula $A$ and state expression $e$, the formula $[A]_{e}$
is defined to equal $A \/ \UNCHANGED e$.  The same reasoning 
\lref{main:unchanged-vars}{used above for an ordered pair of variables}
shows that, if $vars$ is a tuple of variables, then 
  $\UNCHANGED vars$
is true of a step iff the step leaves all those variables unchanged.
Thus, if $vars$ is the tuple of all the specification's variables, a
$[Next]_{vars}$ step is one that is either a $Next$ step or else is a
stuttering step---one that leaves all the specification's variables
unchanged.

If we wanted an algorithm to disallow stuttering steps, we would
have to write its specification as $Init /\ []Next$.  Try it.  Change the
translation's definition of $Spec$ in module $Handshake$ by replacing
$[][Next]_{vars}$ with $[]Next$, and save the module.  You will get
the parsing error:
\begin{display}
$[]$ followed by action not of form $[A]_{v}$.
\end{display}
$[]Next$ is not a legal \tlaplus\ formula.  \tlaplus\ does not let
us write a specification that disallows stuttering steps.  (Run the
translator to restore the original definition of $Spec$.)

We now see why the version of algorithm $Handshake$ with the
additional labels implements algorithm $Alternation$ under our
refinement mapping.  The extra steps introduced by the additional
labels are mapped by the refinement mapping to stuttering steps, which
are allowed by the $Alternation$ algorithm's behavioral specification.

\bigskip

Writing specifications so they allow stuttering steps will seem
strange to most readers.  Why does \tlaplus\ force us to do it?  The
answer is that allowing stuttering steps yields the simplest, most
natural definition of implementation.  To understand why, we must
examine more closely what a \emph{state} is.

In describing our 
  \sref{main}{main:standard-model}{Standard Model},
I wrote that a state is an assignment of values to variables.  I
didn't say \emph{what} variables.  In describing behaviors, I have
described each state of the behavior by stating what values it assigns
to the system's variables.  This would naturally have led you to
believe that a state is an assignment of values just to the system's
variables, so what constitutes a state depends on the system under
consideration.  

I have misled you.  In \tlaplus, 
  \ctindex{1}{state!assigns values to all variables}{state-all-vars}%
a state is an assignment of values to
all of the (infinitely many) possible variables.  Any formula can
contain only a finite number of variables, so it describes only the
values assigned to those variables.  The $Alternation$ specification
mentions only the variables $b$ and $box$.  Hence, whether or
not a behavior $\sigma$ satisfies that specification depends only on
the values assigned to $b$ and $box$ by the states of~$\sigma$.
Those states also assign values to all other possible \tlaplus\
variables: $q$, $r$, $a2\_4xyZ9muuP$, and so on.%
  \marginpop{why-variable-declarations}{Why must we declare
     variables if not to specify what a state's variables are?}
However, the values assigned to those other variables have no effect
on whether or not $\sigma$ satisfies the specification.

Think of a state as specifying a universe that might include the
one-bit clock, the Die Hard system, Euclid's algorithm, and the
Internet.  A behavior satisfying the Die Hard specification is not a
behavior of a system of buckets and water; rather, it is a behavior of
the universe in which the part of the universe that describes how much
water is in the buckets (the variables $big$ and $small$) satisfies
formula $Spec$ of module $PDieHard$.  A specification of the Die Hard
system should not be violated because the one-bit clock ticked
(changing the value of the variable~$b$) between two successive steps
of the Die Hard system.  It isn't violated only because the $DieHard$
specification allows steps in which the value of $b$ and other
variables change while the values of $big$ and $small$ remain the
same.  These are the stuttering steps (the steps satisfying
$\UNCHANGED vars$) allowed by formula $Spec$.

Similarly, whether a behavior $\tau$ should implement the $Alternation$
specification
under the refinement mapping 
  $\ov{b} \deq p\oplus c$, $\ov{box}\deq box$ should depend
only on the changes to the value assigned by the states of $\tau$ to
$\ov{b}$ and $\ov{box}$.  It shouldn't depend on whether or not the
values assigned to $big$ or $small$ changed between changes to
$\ov{b}$ and $\ov{box}$.

Any temporal formula we write should describe only the values of the
variables in that formula.  If a temporal formula $F$ does not contain
the one-bit clock's variable $b$, then whether or not $F$ is true or
false of a behavior $\sigma$ should not depend on whether or not the
one-bit clock ticked between changes to the variables of $F$.  
  \target{suttering-insensitivity}%
Whether
or not $F$ is true of a behavior $\sigma$ should not be changed by
adding or removing steps that do not change any variables of $F$.  A
formula $F$ that has this property is said to be
  \tindex{1}{insensitive to stuttering}%
  \ctindex{1}{stuttering!insensitive to}{stuttering-insensitive}%
\emph{insensitive to stuttering}.  

The syntax of \tlaplus\ allows you to write only formulas that are
insensitive to stuttering.  It doesn't allow you to write the formula
$[]Next$ because adding a stuttering step to a behavior satisfying
this formula could make the formula false on that behavior.  

\bigskip

I have lied to you by writing that a finite behavior represents a
behavior in which the system halts.  When a system halts, the values
assigned to its variables stop changing; the entire universe doesn't
come to a stop.  Hence, an execution in which a system
  \tindex{1}{halting}%
halts is naturally represented as a
behavior that ends with an infinite sequence of states in which the
system's variables remain the same---that is, an infinite sequence of
stuttering steps of that system.  We do not need behaviors containing
only a finite number of states to represent a system that halts, 
  \ctindex{1}{behavior!infinite}{be-inf}%
  \tindex{3}{behavior}%
so we define a behavior to be an infinite sequence of states.  Thus,
when a TLC error trace ends with $<$\textsf{Stuttering}$>$, it is
describing a behavior in which the preceding state is repeated
infinitely many times.

This confession calls for
 \target{main:weak-fairness}%
 \tindex{1}{weak fairness}%
a re-examination of fairness.  Recall that in
  \lref{\xlink{sec:alt-liveness}}{Section~\xref{sec:alt-liveness}},
I wrote that weak fairness of an action $A$ is satisfied by
a behavior $s_{1}-> s_{2} -> \cdots$ iff it satisfies these two conditions:
\begin{itemize}
\item[3a.] The behavior does not end in a state $s_{n}$ in which
$A$ is enabled.

\item[3b.] If the behavior is infinite, then there is no $n$ such
that the infinite behavior $s_{n}->s_{n+1}-> $ has no $A$ steps
but $A$ is enabled in all of its states.
\end{itemize}
As we now know, every behavior is infinite, and ending in a state
$s_{n}$ really means that all the steps in $s_{n}->s_{n+1}->\cdots$
are stuttering steps.  We can therefore combine these conditions into
the single condition:
\begin{itemize}
\item[] There is no $n$ such that \,$s_{n}->s_{n+1}->\cdots$\, has no $A$ steps
but $A$ is enabled in all of its states.
\end{itemize}
There is a problem with this condition.  If $A$ allows stuttering
steps, then the condition is not insensitive to stuttering.  Adding
stuttering $A$ steps from a behavior can make the condition become
true.


To solve this problem, we define 
   \ctindex{1}{+4n@\mmath{\icmd{langle}A\icmd{rangle}_{v}} 
     (action operator)}{+4n}%
$<<A>>_{vars}$ to equal 
 $A /\ (vars' # vars)$.
In other words, an $<<A>>_{vars}$ step is an $A$ step that changes
$vars$.  If $vars$ is the tuple of all system variables, then a step that
leaves $vars$ unchanged is a stuttering step.  Therefore an
$<<A>>_{vars}$ step is a non-stuttering $A$ step.  We
then define the weak fairness formula 
   \tindex{2}{fairness}%
  \ctindex{2}{fairness!weak}{fairness-weak}%
$\WF_{vars}(A)$ by:
\begin{display}
$\WF_{vars}(A)$ is satisfied by a behavior \,$s_{1}->s_{2}->\cdots$\, iff
there is no $n$ such that \,$s_{n}->s_{n+1}->\cdots$\, has no
$<<A>>_{vars}$ step but $<<A>>_{vars}$ is enabled in all of its states.
\end{display}
Weak fairness is an important concept, so you should make sure you
understand this definition.  Observe that it is equivalent to:
\begin{display}
$\WF_{vars}(A)$ asserts of a behavior \,$s_{1}->s_{2}->\cdots$\, that
if $<<A>>_{vars}$ is enabled in all states $s_{m}$ with $m\geq n$ for
some $n$, then there is an $m\geq n$ such that $s_{m}->s_{m+1}$ is an
$<<A>>_{vars}$ step.
\end{display}
This can be expressed informally as: $\WF_{vars}(A)$ asserts that if
$<<A>>_{vars}$ ever becomes enabled forever, then an $<<A>>_{vars}$ step
must eventually occur.  A common situation is one in which an
$<<A>>_{vars}$ step disables $<<A>>_{vars}$---for example, if $A$ is the
$Tick$ or $Tock$ action of algorithm $TickTock$.  In this case, weak
fairness of $<<A>>_{vars}$ implies that the action can never be enabled
forever, since if it were enabled forever, then weak fairness would
imply that an $<<A>>_{vars}$ action must occur, implying that it can't be
enabled forever.  Some people find this observation confusing.  If
you're one of them, you should re-read it until this all becomes
obvious rather than confusing.

\bigskip 

Most of the time, you can forget that a state assigns values to all
variables and think only of a 
  \ctindex{1}{state!system}{state-system}%
  \ctindex{1}{system!state}{system-state}%
\emph{system state}, which is an
assignment of values to the system's variables.  
We will continue to describe behaviors by describing only the system
state, ignoring all the irrelevant variables.  When thinking about a
system implementing a specification, it's important to remember that
a specification allows stuttering steps.  However, we usually ignore
stuttering steps when considering a system in isolation.

For most weak fairness formulas $\WF_{vars}(A)$ that occur in
specifications, action $A$ does not allow stuttering steps that start
in a reachable state.  In that case, we can forget about the
distinction between $A$ and $<<A>>_{vars}$, and I will usually
consider weak fairness of $A$ to be the assertion that is true of a
behavior iff it doesn't end in a sequence of states in which $A$ is
always enabled but no $A$ step occurs.

%try
\subsubsection{A Finer-Grained Algorithm}

Let's add still more steps to the algorithm by putting the label $p3$
on the producer's $p := p\oplus 1$ statement and the label
$c3$ on the consumer's $c := c\oplus 1$ statement.  When we add steps to
an algorithm by adding labels,
we say that the resulting algorithm is 
 \tindex{1}{finer-grained algorithm}%
\emph{finer-grained} than the original, and that the original is
  \tindex{1}{coarser-grained algorithm}%
\emph{coarser-grained}
than the new algorithm.

Run the translator and run TLC to see if this finer-grained algorithm
$Handshake$ still implements $Alternation$ under our refinement mapping.
TLC reports the error:
\begin{widedisplay}
\tt Action property 
  {\darkaqua\underline{line 50, col 20 to line 50, 
       col 32 of module Alternation}} \\
is violated.
\end{widedisplay}
Clicking on the location in the error message shows that the error is
in the subformula $[][Next]_{vars}$ of module $Alternation$.  The
error trace displayed by TLC is the beginning of a behavior $\sigma$
of algorithm $Handshake$ for which the behavior \ov{\sigma} is not a
behavior of algorithm $Alternation$ because it contains a step that is
not a $[Next]_{vars}$ step.  Since TLC reports the shortest possible
error trace for a violation of a safety property, the problem must be
in the last step of that trace.

%try

\popref{trace-explorer}{As you did before}, use TLC's Trace Explorer
to compute the values of $p\oplus c$ (which equals \ov{b}) for the
states in the trace.  You will see that the last step of the error
trace is one in which $box$ (which equals \ov{box}) changes, but
\ov{b} is not changed.  However, in algorithm $Alternation$, $box$ and
$b$ always change together.  Hence the corresponding change to the
variables $box$ and $b$ of module $Alternation$ is not a
$[Next]_{vars}$ step (for $Next$ and $var$ defined in that module).

For the finer-grained $Handshake$ algorithm to implement alternation,
it needs to implement algorithm $Alternation$ under some refinement
mapping for which \ov{box} equals $box$.  It doesn't have to be the
refinement mapping we have been using.  To find such a refinement
mapping, we must define \ov{b} so it changes when $box$ changes.
We do that by defining \ov{b} to equal $vp\oplus vc$, where:
\begin{itemize}
\item[] $vp$ has the same value as $p$ except that it's changed by the 
        execution of statement $p2$ and left unchanged by execution of $p3$.

\item[] $vc$ has the same value as $c$ except it's changed by the 
        execution of statement $c2$ and left unchanged by execution of $c3$.
\end{itemize}
(The $v$ stands for \emph{virtual}.)  We define $vp$ to be that state
function that equals $p$ except when the producer is at control point
$p3$, when it equals $p\oplus 1$; and we define $vc$ similarly.  The
definitions are:
\smallskip
%\begin{widedisplay}
\begin{twocols}[.475]
\begin{notla}
vp == IF pc[0] = "p3" THEN p (+) 1 ELSE p

vc == IF pc[1] = "c3" THEN c (+) 1 ELSE c
\end{notla}
\begin{tlatex}
 \@x{ vp \.{\defeq} {\IF} pc [ 0 ] \.{=}\@w{p3} \.{\THEN} p \.{\oplus} 1
 \.{\ELSE} p}%
\@pvspace{4.0pt}%
 \@x{ vc\@s{0.57} \.{\defeq} {\IF} pc [ 1 ] \.{=}\@w{c3}\@s{0.72} \.{\THEN}
 c\@s{0.57} \.{\oplus} 1 \.{\ELSE} c}%
\end{tlatex}
\midcol
\verb|vp == IF pc[0] = "p3" THEN p (+) 1 ELSE p|\V{.4}
\verb|vc == IF pc[1] = "c3" THEN c (+) 1 ELSE c|
\end{twocols}
\smallskip
%\end{widedisplay}
Add these definitions to the module and add the statement
\begin{display}
$A2 == \INSTANCE Alternation \WITH b <- vp \oplus vc$
\end{display}
Have TLC check property $A2!Spec$.  (Don't forget to remove or uncheck
the property $A!Spec$.)  This time, TLC does not report violation of
an action property.  However, it does report the error:
\begin{display}
\tt Temporal properties were violated.
\end{display}
(If it doesn't, make sure that the two algorithms specify fairness of
the consumer processes and not the producer processes.)

The error trace shows a behavior $\sigma$ that stops (ends in an
infinite sequence of stuttering steps) with the producer at $p3$.
This behavior is allowed because there is no fairness requirement for
the producer, so it can stop taking steps, and the consumer can take
no step (its next-state action is not enabled) because it is at $c1$
and $p#c$ equals $\FALSE$.  However, in the behavior \ov{\sigma}
corresponding to $\sigma$ under the refinement mapping, the producer
has performed its $p1$ action and the consumer's next-state action is
enabled, but the consumer does nothing.  Hence, the consumer's
fairness requirement is not satisfied by \ov{\sigma}, so $\sigma$ is a
behavior of (the finer-grained) algorithm $Handshake$ but \ov{\sigma}
is not a behavior of algorithm $Alternation$.

To make 
 \target{label-decoration}%
the the finer-grained $Handshake$ algorithm implement
alternation, we must add a fairness requirement on the producer that
prevents it from stopping with control at $p3$.  We can do this by
requiring weak fairness of action $p3$---that is, by adding the
conjunct $\WF_{vars}(p3)$ to the definition of $Spec$.  To specify
this requirement in the PlusCal code, we specify weak fairness of the
producer process except for its $p1$ and $p2$ actions.   
  \ctindex{1}{+2pl@\mmath{-} after PlusCal label}{+2pl}%
We exempt an action from a fairness requirement by putting a dash
(\texttt{-}) after its label.  (Spaces before and after the ``\,:\,''
are allowed but not required.)  So, we change the producer process to:
\begin{display}
\begin{nopcal}
fair process (Producer = 0) 
  { p1:- while (TRUE)
          {      await p = c ;
            p2:- box := Put(box) ;
                 p3: p := p (+) 1
          }
  }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@fair} {\p@process} {\p@lparen} Producer \.{=} 0 {\p@rparen}}%
 \@x{\@s{8.2} {\p@lbrace} p1\@s{.5}\textrm{:-}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{42.23} {\p@lbrace}\@s{20.5} {\p@await} p \.{=} c {\p@semicolon}}%
 \@x{\@s{50.43} p2\@s{.5}\textrm{:-}\@s{3}\@s{1.52} box \.{:=} Put ( box )
 {\p@semicolon}}%
\@x{\@s{72.32} p3\@s{.5}\textrm{:}\@s{3} p \.{:=} p \.{\oplus} 1}%
\@x{\@s{42.23} {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\end{display}
This causes the translation to add the following conjunct to the definition
of $Next$:
 \[ \WF_{vars}((pc[0] \notin \{"p1",\, "p2"\}) /\ Producer) \]
This is equivalent to $\WF_{vars}(p3)$ because
the definitions of $Producer$, $p1$, $p2$, and $p3$ imply that
 $(pc[0] \notin \{"p1",\, "p2"\}) /\ Producer$
is equivalent to $p3$.  TLC verifies that, with this additional fairness
requirement, algorithm $Handshake$ implements algorithm $Alternation$
under the refinement mapping.


\subsection{Temporal Logic and Refinement} \xlabel{main:tl-refinement}

We now examine what refinement means in terms of temporal logic.
Let's start by considering what it means for a system to satisfy a
property, beginning with the simple property of invariance of a state
predicate.  Remember that a temporal formula $F$ is an assertion about
behaviors, meaning that it is true or false of a behavior.  We often
say that a behavior $\sigma$ 
  \tindex{1}{satisfies}%
\emph{satisfies} $F$ if $F$ is true of
$\sigma$.  A 
   \tindex{1}{theorem, temporal}%
temporal formula that is satisfied by all behaviors is
called a \emph{theorem},
  \marginpar{\popref{true-vs-provable}{Theorems versus provable formulas.}}
or simply a \emph{true formula}.

A state predicate $I$ is an invariant of a specification $Spec$ iff it
is true in all states of every behavior $\sigma$ that satisfies
$Spec$.  The temporal formula $[]I$ is true of $\sigma$ iff $I$ is
true in all states of $\sigma$.  Thus, $I$ is an invariant of $Spec$
iff, for any behavior $\sigma$, if $\sigma$ satisfies $Spec$ then
$\sigma$ satisfies $[]I$.  This is the case iff every behavior
$\sigma$ satisfies the formula $Spec =>[]I$.  In other words, $I$ is
an invariant of $Spec$ iff the formula $Spec =>[]I$ is a theorem.
  
In general, a property $P$ is a temporal formula.  A specification
$Spec$ is said to \emph{satisfy} $P$ iff $Spec => P$ is a theorem.
Invariance of a state predicate $I$ means that $Spec$ satisfies
the property $[]I$.

Now let's consider implementation under a refinement mapping.  For
concreteness, let's consider the implementation of algorithm
$Alternation$ by algorithm $Handshake$ of
\lref{\xlink{ref-and-stutter}}{Section~\xref{ref-and-stutter}} under
the refinement mapping $\ov{b}\,\deq\,p\oplus c$,
$\ov{box}\,\deq\,box$.  
%
% We ignore fairness for now, so we consider the algorithms with no
% fairness requirements.
%
Using the same name for different formulas gets confusing, so let's
use subscripts to indicate in which specification an expression is
defined.  For example, let $Init_{A}$ be the formula $Init$ defined in
the $Alternation$ spec, and let $vars_{H}$ be expression $vars$
defined in the $Handshake$ specification.

Now that I've confessed that a state is an assignment of values to
\emph{all} variables, I should restate what it means for algorithm
$Handshake$ to implement algorithm $Alternation$ under the refinement
mapping---without talking about states of a system.  For any state
$s$, we define \ov{s} to be some state that assigns the value \ov{b}
to the variable $b$ and the value $\ov{box}$ to the variable $box$.
It doesn't matter what values \ov{s} assigns to other variables.  For
any behavior \,$s_{1}->s_{2}->\cdots$\,, we define
  \,\ov{s_{1}\rightarrow s_{2}\rightarrow\cdots}\, 
to be the behavior \,$\ov{s_{1}}->\ov{s_{2}}->\cdots$\,.
Implementation under the refinement mapping means that if $\sigma$ is
any behavior satisfying $Spec_{H}$, then the behavior \ov{\sigma}
satisfies $Spec_{A}$.

For any formula $F_{A}$ defined in the $Alternation$ spec, define
\ov{F_{A}} to be the formula obtained from $F_{A}$ by substituting
$\ov{b}$ for $b$ and $\ov{box}$ for $box$.  (Of course, the latter
substitution does nothing because $\ov{box}$ equals $box$ for this
particular refinement mapping.)  For example, we have
\begin{display}
\begin{twocols}[.37]
$Init_{A} == \begin{conj}
              b = 0 \\
              box = << >>             
             \end{conj}$\V{.6}
$Consumer_{A} == \begin{conj}
             b = 1 \\
             box' = Tail(box) \\
             b' = 0
             \end{conj}$
\midcol
$\ov{Init_{A}} == \begin{conj}
              (p\oplus c) = 0 \\
              box = << >>             
             \end{conj}$\V{.4}
$\ov{Consumer_{A}} == \begin{conj}
             p\oplus c = 1 \\
             box' = Tail(box) \\
             (p\oplus c)' = 0
             \end{conj}$
\end{twocols}
\end{display}
where the definition of $Get_{A}$ has been expanded in the definition
of $Consumer_{A}$.
For any step $s->t$:
\begin{describe}{iff}
\item[] 
$Consumer_{A}$ is true on the step $\ov{s}->\ov{t}$ 

\item[iff]
 $Consumer_{A}$ is true with $b$ and $box$ replaced by their values
  in state \ov{s} and $b'$ and $box'$ replaced by the values
   of $b$ and $box$ 
  in state \ov{t} \\ {\small [by definition of what it means for
  an action to be true on a step]}

\item[iff]
 $Consumer_{A}$ is true with $b$ and $box$ replaced by the values
  of \ov{b} and \ov{box} in state $s$
and $b'$ and $box'$ replaced by the values of \ov{b} and \ov{box}
  in state $t$\\
  {\small [by definition of \ov{s} and \ov{t}]}
  
\item[iff]
  \ov{Consumer_{A}} is true on the step $s->t$.\\ 
  {\small [by definition of what it means for
  an action to be true on a step]}
\end{describe}
It's clear that this is a general result for any action or state
predicate.  In particular $Next_{A}$ is true on $\ov{s}->\ov{t}$ iff
\ov{Next_{A}} is true on $s->t$, and $Init_{A}$ is true on a state
\ov{s} iff \ov{Init_{A}} is true on~$s$.  

The analogous result holds for temporal formulas.  In particular, if
$\sigma$ is any behavior, $\ov{\sigma}$ satisfies $Spec_{A}$ iff
$\sigma$ satisfies \ov{Spec_{A}}.  Remember that implementation under
the refinement mapping means that for any behavior $\sigma$, if $\sigma$
satisfies $Spec_{H}$, then \ov{\sigma} satisfies $Spec_{A}$.  Thus, it
means that for any behavior $\sigma$, if $\sigma$ satisfies
$Spec_{H}$, then $\sigma$ satisfies \ov{Spec_{A}}.  In other words,
algorithm $Handshake$ implements algorithm $Alternation$ under the
refinement mapping means that $Spec_{H}$ satisfies the property
\ov{Spec_{A}}, which means that $Spec_{H} => \ov{Spec_{A}}$ is a
theorem.

%try
How do we prove the theorem $Spec_{H} => \ov{Spec_{A}}$\,?  The definitions
of $Spec_{H}$ and $Spec_{A}$ are:
 \[ \begin{noj3}
    Spec_{H} & == & Init_{H} \,/\ \, [][Next_{H}]_{vars_{H}} 
                        \,/\ \, \WF_{vars_{H}}(Consumer_{H}) \V{.2}
    Spec_{A} & == & Init_{A} \,/\ \, [][Next_{A}]_{vars_{A}} 
                        \,/\ \, \WF_{vars_{A}}(Consumer_{A})
    \end{noj3}
 \]
Since overbarring a formula means substituting expressions for variables
in the formula, it's clear that \ov{Spec_{A}} equals%
\marginpar{\popref{wfbar}{Why\V{.4} 
\s{1}$\ov{\WF_{{vars_{A}}}({Consumer_{A}})}$\V{.4}
  and not\V{.2}
 \s{1}${\WF_{\ov{vars_{A}}}(\ov{Consumer_{A}})}$\,?}}
 \[
  \ov{Init_{A}} \,/\ \, [][\ov{Next_{A}}]_{\ov{vars_{A}}} 
                        \,/\ \, \ov{\WF_{{vars_{A}}}({Consumer_{A}})}
 \]
From this, it follows that to prove $Spec_{H} => \ov{Spec_{A}}$,
it suffices to prove:
\begin{describe}{1.}
\item[1.] $Spec_{H} =>\ov{Init_{A}}$
\item[2.] $Spec_{H} => [][\ov{Next_{A}}]_{\ov{vars_{A}}}$
\item[3.] $Spec_{H} => \ov{\WF_{{vars_{A}}}({Consumer_{A}})}$
\end{describe}
We prove~1 by proving:
\begin{describe}{R1.}
\item[R1.] $Init_{H} => \ov{Init_{A}}$
\end{describe}
This follows easily from the definition of \ov{Init_{A}} given above
and the definition of $Init_{H}$ produced by the translator, since
$0\oplus 0$ equals~0.  
%try

The obvious way to prove formula~2 is to prove
\begin{describe}{$2a$.}
\item[$2a$.]   $[][Next_{H}]_{vars_{H}} \,=>\, [][\ov{Next_{A}}]_{\ov{vars_{A}}}$
\end{describe}
whose truth asserts that, for every behavior $\sigma$, if every
step of $\sigma$ is a $[Next_{H}]_{vars_{H}}$ step, then 
every step of $\sigma$ is a $[\ov{Next_{A}}]_{\ov{vars_{A}}}$
step.  This is true if it is true that every $[Next_{H}]_{vars_{H}}$
is a $[\ov{Next_{A}}]_{\ov{vars_{A}}}$ step.  In other words,~we
can prove $2a$ by proving:
\begin{describe}{$2b$.}
\item[$2b$.]  $[Next_{H}]_{vars_{H}} \,=>\, [\ov{Next_{A}}]_{\ov{vars_{A}}}$
\end{describe}
Because $[Next_{H}]_{vars_{H}}$ equals
 $Next_{H} \/ \UNCHANGED {vars_{H}}$, to prove
$2b$ it suffices to prove:
\begin{describe}{$2b1$.}
\item[$2b1.$] $Next_{H} \,=>\, [\ov{Next_{A}}]_{\ov{vars_{A}}}$

\item[$2b2$.] $\UNCHANGED {vars_{H}} \,=>\, [\ov{Next_{A}}]_{\ov{vars_{A}}}$
\end{describe}
\begin{sloppypar} \noindent
Condition $2b2$ is obviously true, since
$[\ov{Next_{A}}]_{\ov{vars_{A}}}$ equals $[\ov{Next_{A}}] \/
\UNCHANGED {\ov{vars_{A}}}$, and $\UNCHANGED {vars_{H}}$ implies
$\UNCHANGED {\ov{vars_{A}}}$ because $\UNCHANGED {vars_{H}}$ asserts
that all the variables of specification $Handshake$ are unchanged,
which implies that \ov{v} is unchanged for all the (two) variables $v$
of $Alternation$.  So, we just have to prove $2b1$.
\end{sloppypar}

To prove $2b1$, we must prove that it is true for all steps
$s->t$---even ones in $s$ and $t$ assign non-numerical values to $p$
and $c$ states.  This is impossible---for example, we don't know what
$p\oplus 1$ equals if $p$ equals $"abc"$.  To prove~2, it suffices to
prove that formula $2b1$ is true for all steps $s->t$ in which $s$ and
$t$ are reachable states of $Spec_{H}$---that is, states that can
occur in a behavior satisfying $Spec_{H}$.  Any invariant of
$Spec_{H}$ is true in all reachable states.  Therefore, to
prove that $2b1$ is true for steps $s->t$ where $s$ and $t$ are
reachable, it suffices to prove:
\begin{describe}{R2.}
\item[R2.] $Inv_{H} \,/\ \, {Inv_{H}}' \, /\ \, Next_{H} \;=>\;
[\ov{Next_{A}}]_{\ov{vars_{A}}}$ 
\s{1}where $Spec_{H} => []Inv_{H}$ is true.\s{-10}
\end{describe}
%
For our example, the invariant we need is
 \[ Inv_{H} == \begin{conj}
                p \in \{0,1\} \\
                c \in \{0,1\} \\
                (pc[0] = "p2") => (p = c) \\
                (pc[1] = "c2") => (p # c) 
               \end{conj}
 \]
\popref{r2-handshake}{Here is the proof of R2}.  

Writing proofs of liveness properties will become easier when you
learn a little more temporal logic.  I will therefore defer an
explanation of how to prove condition~3.

\begin{aquestion}{question-hs-proof}
Show that the formula $Inv_{H}$ defined above is an invariant of the
$Handshake$ specification.  Why isn't it an inductive invariant?
How can we strengthen it to obtain an inductive invariant?
\end{aquestion}
%
Let's now return to the \textsc{instance} statement
\begin{display}
$A == \INSTANCE Alternation \WITH b <- p \oplus c,\  box <- box$
\end{display}
and the formula $A!Spec$ that we added to the \textsf{Properties}
subsection of the TLC model.  For every symbol $F$ defined in module
$Alternation$, this statement defines $A!F$%
 \marginpar{\popref{bang-notation}{What does ``\,!\,'' mean?}}%
 \ctindex{1}{+5t@\mmath{"!} (with instantiation)}{+5t} %"
to be what we have been
writing \ov{F}.  Thus, $A!Spec$ equals what we have been writing
\ov{Spec_{A}}.  Making $A!Spec$ a property of the model to be checked
tells TLC to check that the formula $Spec => A!Spec$ is true, where
$Spec$ is the specification selected by the model's \textsf{What is
the behavior spec?} section.  Thus, telling TLC to check the property
$[]I$, where $I$ is a state predicate, tells it to check that $Spec =>
[]I$ is true.  It is equivalent to adding $I$ to the
\textsf{Invariants} section of the model.

If $F$ is defined in module $Alternation$ to be an operator with
the definition:
 \[ F(p_{1}, \ldots, p_{n}) == exp \]
then $A!F$ is defined in module $Handshake$ by
 \[  A!F(p_{1}, \ldots, p_{n}) == \ov{exp}
 \]
where \ov{exp} is the expression obtained from $exp$ by replacing each
variable $v$ of modle $Alternation$ with \ov{v}.  For example, if module
$Alternation$ contained the definition:
 \[ ProCon(i) == \begin{conj}
                 b = i \\ b' = i \oplus 1
                 \end{conj} 
 \]
then the \textsc{instance} statement would effectively define
  \[ A!ProCon(i) == \begin{conj}
                 p \oplus c = i \\ (p\oplus c)' = i \oplus 1
                 \end{conj} 
  \]
in module $Handshake$.  (Of course, we can't actually write that
definition because $A!ProCon$ is not a legal identifier.)  Note that
because the definitions of $Put$ and $Get$ in module $Alternation$
contain no variables, the \textsc{instance} statement defines $A!Put$
and $A!Get$ in module $Handshake$ to be the same as the operators
$Put$ and $Get$ defined in that model.

\bigskip

Everything we have done here generalizes to proving that any safety
specification (one with no fairness conditions) implements another
safety specification under a refinement mapping.  Let me restate it:
\begin{display}
To 
 \target{r1r2}%
prove that $Init_{H} /\ [][Next_{H}]_{vars_{H}}$ implements
 $Init_{A} /\ [][Next_{A}]_{vars_{A}}$ under a refinement mapping,
we prove
  \[ Init_{H} /\ [][Next_{H}]_{vars_{H}} \;=>\;
     \ov{Init_{A} /\ [][Next_{A}]_{vars_{A}}}
 \]
by finding an invariant $Inv_{H}$ of $Init_{H} /\ [][Next_{H}]_{vars_{H}}$
(a formula for which $Init_{H} /\ [][Next_{H}]_{vars_{H}} \;=>\; []Inv_{H}$
is a theorem) and proving:
\begin{enumerate}%{R2.}
\item[R1.] $Init_{H} => \ov{Init_{A}}$

\item[R2.] $Inv_{H} \,/\ \, {Inv_{H}}' \, /\ \, Next_{H} \;=>\;
[\ov{Next_{A}}]_{\ov{vars_{A}}}$
\end{enumerate}
\end{display}
Condition R2 is called 
  \ctindex{1}{step simulation}{step-simulation}%
  \ctindex{1}{simulation, step}{simulation-step}%
\emph{step simulation}.  It asserts that any $Next_{H}$ step simulates,
under the refinement mapping, either a $Next_{A}$ step or a stuttering
step.  

To prove that $Inv_{H}$ is an invariant, we have to find an inductive
invariant that implies it.  We usually take $Inv_{H}$ to be that
inductive invariant.

\subsection{Alternation Revisited}  \xlabel{alt-revisited}

The main purpose of this discussion of alternation and the two-phase
handshake protocol has been to introduce the concepts of refinement
and insensitivity to stuttering.  Now that you understand those
concepts, we can describe the alternation problem in a somewhat more
conventional way.  Such syncronization problems are usually expressed
in terms of pseudo-code.  We will do it in terms of PlusCal.

Let's ignore variable declarations and consider only the two
processes.  We are given two arbitrary pieces of PlusCal code, $put$
and $get$, which may 
  \marginpar{PlusCal has a \textbf{goto} statement that can jump
   out from the middle of a piece of code.}% 
contain labels, but can exit only by ``falling
off the bottom''.  
They must be executed so that first $put$ is executed, then $get$ is
executed, then $put$ is executed, and so on.  We must achieve this by
implementing code sections $p\_enter$, $p\_exit$, $c\_enter$ and
$c\_exit$, using no variables that appear in $put$ or $get$, in these
two processes:
\begin{display}
\begin{twocols}[.4]
\begin{nopcal}
process (Producer = 0) 
  { pe: while (TRUE) 
          {      p_enter ;
            p:   put ;
            px:  p_exit     }
  }
\end{nopcal}      
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} Producer \.{=} 0 {\p@rparen}}%
 \@x{\@s{8.2} {\p@lbrace} pe\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{42.21} {\p@lbrace}\@s{20.5} p\_enter {\p@semicolon}}%
\@x{\@s{50.41} p\@s{.5}\textrm{:}\@s{3}\@s{9.86} put {\p@semicolon}}%
 \@x{\@s{50.41} px\@s{.5}\textrm{:}\@s{3}\@s{4.65} p\_exit\@s{16.4}
 {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\midcol
\begin{nopcal}
process (Consumer = 1)
  { ce: while (TRUE)
          {      c_enter ;
            g:   get ; 
            cx:  c_exit        }
  }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} Consumer \.{=} 1 {\p@rparen}}%
 \@x{\@s{8.2} {\p@lbrace} ce\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{41.70} {\p@lbrace}\@s{20.5} c\_enter {\p@semicolon}}%
\@x{\@s{49.90} g\@s{.5}\textrm{:}\@s{3}\@s{10.12} get {\p@semicolon}}%
 \@x{\@s{49.90} cx\@s{.5}\textrm{:}\@s{3}\@s{5.16} c\_exit\@s{28.7}
 {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\end{twocols}
\end{display}
Let's call this generic two-process algorithm $AltImpl$.

A trivial solution is the algorithm containing the following
processes, where we assume that the variable $b$ does not occur
in $put$ or $get$.
\begin{display}
\begin{twocols}[.4]
\begin{nopcal}
process (Producer = 0) 
  { pe: while (TRUE) 
          {      await b = 0 ;
            p:   put ;
            px:  b := 1         }  
  }
\end{nopcal}      
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} Producer \.{=} 0 {\p@rparen}}%
 \@x{\@s{8.2} {\p@lbrace} pe\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{42.21} {\p@lbrace}\@s{20.5} {\p@await} b \.{=} 0 {\p@semicolon}}%
\@x{\@s{50.41} p\@s{.5}\textrm{:}\@s{3}\@s{9.86} put {\p@semicolon}}%
 \@x{\@s{50.41} px\@s{.5}\textrm{:}\@s{3}\@s{4.65} b \.{:=} 1\@s{32.8}
 {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\midcol
\begin{nopcal}
process (Consumer = 1)
  { ce: while (TRUE)
          {      await b = 1 ;
            g:   get ; 
            cx:  b := 0        }
  }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} Consumer \.{=} 1 {\p@rparen}}%
 \@x{\@s{8.2} {\p@lbrace} ce\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{41.70} {\p@lbrace}\@s{20.5} {\p@await} b \.{=} 1 {\p@semicolon}}%
\@x{\@s{49.90} g\@s{.5}\textrm{:}\@s{3}\@s{10.12} get {\p@semicolon}}%
 \@x{\@s{49.90} cx\@s{.5}\textrm{:}\@s{3}\@s{5.16} b \.{:=} 0\@s{28.7}
 {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\end{twocols}
\end{display}
We can take this algorithm, which we call $AltSpec$, to be the
specification of the alternation problem.  An algorithm $AltImpl$
is an alternation solution if it implements $AltSpec$ under a
refinement mapping with the following properties:
\begin{enumerate}
\item \ov{v} equals $v$ for every variable $v$ that occurs in $put$
or $get$.  

\item \ov{pc} satisfies:
\begin{describe}{\ $\bullet$}
\item[\ $\bullet$] $\ov{pc}[0]$ equals $"pe"$ if $pc[0]$ equals $"pe"$ or one of
the label names in $p\_enter$.

\item[\ $\bullet$] $\ov{pc}[0]$ equals $pc[0]$ if $pc[0]$ equals $"p"$ or one of
the label names in $put$.

\item[\ $\bullet$] $\ov{pc}[0]$ equals $"px"$ if $pc[0]$ equals $"px"$ or one of
the label names in $p\_exit$.
\end{describe}
and the analogous three conditions for $\ov{pc}[1]$.
\end{enumerate}
We want the solution $AltImpl$ to work for any $put$ and $get$ code.
It's easy to see that this will be the case if the solution works when
$put$ and $get$ are just \textbf{skip} statements, where
    \ctindex{1}{skip (PlusCal statement)@\icmd{textbf}{skip} (PlusCal statement)}{skip}%
a \textbf{skip} statement leaves all variabes (except $pc$) unchanged.
We simplify the problem by replacing $put$ and $get$ with
\textbf{skip} statements.  The resulting algorithms are abstractions
in which an execution of $put$ or $get$ is represented as a sequence
of stuttering steps followed by execution of the \textbf{skip} (which
modifies $pc$).  


Our specification of alternation is then \popref{altspec}{this
algorithm $AltSpec$}, in which we have changed the labels $p$ and $g$
to $put$ and $get$.  Create a new specification $AltSpec$ and
add \popref{altspec-ascii}{the \textsc{ascii} version of the algorithm}
to it.  Run the translator and observe the translation of the
\textbf{skip} statements.  A solution to the alternation problem is
a two-process algorithm with these processes
\begin{display}
\begin{twocols}[.4]
\begin{nopcal}
process (Producer = 0) 
  { pe: while (TRUE) 
          {      p_enter ;
            put: skip ;
            px:  p_exit     }
  }
\end{nopcal}      
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} Producer \.{=} 0 {\p@rparen}}%
 \@x{\@s{8.2} {\p@lbrace} pe\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{42.21} {\p@lbrace}\@s{20.5} p\_enter {\p@semicolon}}%
\@x{\@s{50.41} put\@s{.5}\textrm{:}\@s{3}\@s{0.85} {\p@skip} {\p@semicolon}}%
 \@x{\@s{50.41} px\@s{.5}\textrm{:}\@s{3}\@s{4.65} p\_exit\@s{16.4}
 {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\midcol
\begin{nopcal}
process (Consumer = 1)
  { ce: while (TRUE)
          {      c_enter ;
            get: skip ; 
            cx:  c_exit        }
  }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} Consumer \.{=} 1 {\p@rparen}}%
 \@x{\@s{8.2} {\p@lbrace} ce\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{41.70} {\p@lbrace}\@s{20.5} c\_enter {\p@semicolon}}%
\@x{\@s{49.90} get\@s{.5}\textrm{:}\@s{3}\@s{2.13} {\p@skip} {\p@semicolon}}%
 \@x{\@s{49.90} cx\@s{.5}\textrm{:}\@s{3}\@s{5.16} c\_exit\@s{28.7}
 {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\end{twocols}
\end{display}
that implements algorithm $AltSpec$ under a refinement mapping 
satisfying the following conditions:
\begin{itemize}
\item $\ov{pc}[0]$ equals $"pe"$ if $pc[0]$ equals $"pe"$ or one of
the label names in $p\_enter$.

\item $\ov{pc}[0]$ equals $"put"$ if $pc[0]$ equals $"put"$.

\item $\ov{pc}[0]$ equals $"px"$ if $pc[0]$ equals $"px"$ or one of
the label names in $p\_exit$.
\end{itemize}
and the corresponding three conditions for $\ov{pc}[1]$.

\begin{aquestion}{hs-impl-answer}
 \targetlabel{hs-impl-question}%
Rewrite \popref{handshake}{the two-phase handshake algorithm} (by modifying
\popref{handshake-ascii}{its \textsc{ascii} version}) so it is a
solution to the new statement of the alternation problem.  Use TLC
to check that it implements algorithm $AltSpec$ under the appropriate
refinement mapping.
\end{aquestion}

Let's make the two-phase handshake example a little more interesting
by writing a finer-grained version that splits the evaluation of
each \textbf{await} test into two separate actions: reading the
other process's variable and then testing its value.  This is done
in the following code, which introduces two features of PlusCal:
   \ctindex{1}{goto (PlusCal statement)@\icmd{textbf}{goto} (PlusCal statement)}{goto}%
\textbf{goto} statements and 
  \ctindex{1}{variable!process-local}{variable-process-local}%
  \ctindex{1}{process!variable local to}{process-variable-local}%
process-local variables.  If you don't already know what a
\textbf{goto} statement does, its \tlaplus\ translation will explain
it.
\begin{display}
\begin{twocols}[.45]
\begin{nopcal}
process (Producer = 0) 
  variable tp = 0 ;
  { pe: while (TRUE)
          {      tp := c ;
            pe1: if (p # tp) {goto pe} ; 
            put: skip ;
            px:  p := p (+) 1   }
  }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} Producer \.{=} 0 {\p@rparen}}%
\@x{\@s{8.2} {\p@variable} tp \.{=} 0 {\p@semicolon}}%
 \@x{\@s{8.2} {\p@lbrace} pe\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{42.21} {\p@lbrace}\@s{20.5} tp \.{:=} c {\p@semicolon}}%
 \@x{\@s{50.41} pe1\@s{.5}\textrm{:}\@s{3}\@s{0.65} {\p@if} {\p@lparen} p
 \.{\neq} tp {\p@rparen} {\p@lbrace} {\p@goto} pe {\p@rbrace} {\p@semicolon}}%
\@x{\@s{50.41} put\@s{.5}\textrm{:}\@s{3}\@s{0.85} {\p@skip} {\p@semicolon}}%
 \@x{\@s{50.41} px\@s{.5}\textrm{:}\@s{3}\@s{4.65} p \.{:=} p \.{\oplus}
 1\@s{8.2} {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\midcol
\begin{nopcal}
process (Consumer = 1)
  variable tc = 0 ;
  { ce: while (TRUE)
          {      tc := p ; 
            ce1: if (c = tc) {goto ce} ;
            get: skip ;
            cx:  c := c (+) 1   }
  }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} Consumer \.{=} 1 {\p@rparen}}%
\@x{\@s{8.2} {\p@variable} tc \.{=} 0 {\p@semicolon}}%
 \@x{\@s{8.2} {\p@lbrace} ce\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{41.70} {\p@lbrace}\@s{20.5} tc \.{:=} p {\p@semicolon}}%
 \@x{\@s{49.90} ce1\@s{.5}\textrm{:}\@s{3}\@s{1.16} {\p@if} {\p@lparen} c
 \.{=} tc {\p@rparen} {\p@lbrace} {\p@goto} ce {\p@rbrace} {\p@semicolon}}%
\@x{\@s{49.90} get\@s{.5}\textrm{:}\@s{3}\@s{2.13} {\p@skip} {\p@semicolon}}%
 \@x{\@s{49.90} cx\@s{.5}\textrm{:}\@s{3}\@s{5.16} c \.{:=} c \.{\oplus}
 1\@s{8.2} {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\end{twocols}
\end{display}
We could use the same name for the two process's local variables, but
one of them would be renamed in the \tlaplus\ translation.  The
initial values of the local variables $tp$ and $tc$ are irrelevant;
we'll see later what happens if we don't specify initial values for them.

We now must define the refinement mapping to show that the algorithm
with these process declarations is an alternation solution.  The
refinement mapping again defines \ov{b} to equal $p\oplus c$.  The
definition of \ov{pc} follows directly from the conditions that it
must satisfy.  Read and understand the following definition, which
defines $pcBar$ to equal \ov{pc}.  (It is written with the
   \ctindex{2}{case (expression)@\icmd{textsc}{case} (expression)}{case-expr}%
\rref{math}{case-expr}{\textsc{case} construct} instead of with
\textsc{if}\,/\,\textsc{then}\,/\,\textsc{else} to make it more
obviously symmetric in the process id.)
\begin{display}
\begin{notla}
pcBar == [i \in {0, 1} |-> CASE i = 0 -> IF pc[0] = "pe1" THEN "pe"
                                                          ELSE pc[0]
                             [] i = i -> IF pc[1] = "ce1" THEN "ce"
                                                          ELSE pc[1] ]
\end{notla}
\begin{tlatex}
 \@x{ pcBar \.{\defeq} [ i \.{\in} \{ 0 ,\, 1 \} \.{\mapsto} {\CASE} i \.{=} 0
 \.{\rightarrow} {\IF} pc [ 0 ] \.{=}\@w{pe1} \.{\THEN}\@w{pe}}%
  \ascii{hs-pcbar-ascii}%
\@x{\@s{242.04} \.{\ELSE} pc [ 0 ]}%
 \@x{\@s{115.59} {\Box}\@s{10.30} i \.{=} i\@s{0.91} \.{\rightarrow} {\IF} pc
 [ 1 ] \.{=}\@w{ce1}\@s{1.00} \.{\THEN}\@w{ce}}%
\@x{\@s{242.04} \.{\ELSE} pc [ 1 ] ]}%
\end{tlatex}
\end{display}

\begin{question}
Modify the module you wrote as \popref{hs-impl-answer}{the answer to
Question~\xref{hs-impl-question}} to have these finer-grained
processes, and add this definition of $pcBar$ to it.  Add an
\textsc{instance} statement that instantiates module $AltSpec$ under
the refinement mapping
  \,$b <- p \oplus c\,, \ pc <- pcBar$\,
and have TLC check that the specification implements $AltSpec$ under
this refinement mapping.
\end{question}

Remove the initialization of the process local variables $tp$ and $tc$
(by removing the ``\texttt{= 0}''s) and rerun the translator.  Observe
that the translation now asserts that $tp$
and $tc$ equal the declared constant 
  \ctindex{1}{defaultInitValue@\mmath{defaultInitValue}}{defaultInitValue}%
$defaultInitValue$.  For TLC to check a \tlaplus\ specification, the
initial predicate must specify an initial value, or a set of possible
initial values, for each variable.  The translation uses the
unspecified constant $defaultInitValue$ as the initial value of any
variable whose initial value is not specified in its \textbf{variable}
declaration.  
Now create a new TLC model of the specification.  You will see that
the model has set $defaultInitValue$ to be a
\helppage{model/model-values}{\emph{model value}}.

\begin{aquestion}{init-array}
Suppose a PlusCal algorithm contains a global variable $f$ for which
there is an assignment statement of the form $f[e] := \ldots$\,.  Why
does the algorithm probably have to specify the initial value (or set
of possible initial values) of $f$?
\end{aquestion}


 \tindex{1}{round robin synchronization}%
\subsection{Round-Robin Synchronization} \xlabel{round-robin}

We now generalize from alternation to round-robin synchronization in
which there are $N$ processes numbered 0 through $N-1$, where each
process $i$ executes an operation $Op(i)$.  An algorithm must ensure
that these operations are executed in round-robin order:
 \[ Op(0) \,->\, Op(1) \,->\, \,\cdots\, \,->\, Op(N-1) \,->\, 
Op(0) \,->\, Op(1) \,->\, \,\cdots \]
To see how to state the more general problem, we first rewrite
the one-bit clock algorithm.

\subsubsection{The One-Bit Clock Revisited Again}

In \lref{\xlink{one-bit-revisited}}{Section~\xref{one-bit-revisited}},
we saw how to describe the one-bit clock as a two-process PlusCal
algorithm.  The module $TickTock$ represented each process by a
\textbf{process} declaration.  We now write the same specification
in a slightly different way, using a single process-set declaration
to describe both processes.

In the Toolbox, create a new specification named $TickTock2$, and
add an \,$\EXTENDS Integers$\, statement.
% 
% Have it begin by importing the $Integers$ module and defining the
% operator $(+)$ as in the handshake algorithm's specification.
% \begin{display}
% \begin{twocols}[.35]
% $\EXTENDS Integers$\V{.4}
% $a \oplus b == (a + b) \,\%\, 2$
% \midcol
% \verb|EXTENDS Integers| \V{.4}
% \verb|a (+) b == (a + b) % 2|
% \end{twocols}
% \end{display}
%
Now add \popref{ticktock2-ascii}{this PlusCal algorithm}, which
contains this \textbf{process} statement:

\begin{display}
\begin{nopcal}
process (TickTock \in {0, 1}) 
      { t: while (TRUE)
              { await b = self ;
                b := (self + 1)  %  2 ;
              }
      }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} TickTock \.{\in} \{ 0 ,\, 1 \} {\p@rparen}}%
 \@x{\@s{24.59} {\p@lbrace} t\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{57.03} {\p@lbrace} {\p@await} b \.{=} self {\p@semicolon}}%
 \@x{\@s{66.61} b \.{:=} ( self \.{+} 1 )\@s{4.1} \.{\%}\@s{4.1} 2
 {\p@semicolon}}%
\@x{\@s{57.03} {\p@rbrace}}%
\@x{\@s{24.59} {\p@rbrace}}%
\end{tlatex}
\end{display}
It defines two processes having the process ids 0 and 1.  The identifier
$self$ in the body of the process denotes the process's id.  Thus,
this \textbf{process} statement is equivalent to these two separate
\textbf{process} statements:
\begin{display}
\begin{twocols}[.4]
\begin{nopcal}
process (TickTock0 = 0) 
      { t: while (TRUE)
              { await b = 0 ;
                b := (0 + 1)  %  2 ;
              }
      }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} TickTock0 \.{=} 0 {\p@rparen}}%
 \@x{\@s{24.59} {\p@lbrace} t\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{57.03} {\p@lbrace} {\p@await} b \.{=} 0 {\p@semicolon}}%
 \@x{\@s{66.61} b \.{:=} ( 0 \.{+} 1 )\@s{4.1} \.{\%}\@s{4.1} 2
 {\p@semicolon}}%
\@x{\@s{57.03} {\p@rbrace}}%
\@x{\@s{24.59} {\p@rbrace}}%
\end{tlatex}
\midcol
\begin{nopcal}
process (TickTock1 = 1) 
      { t: while (TRUE)
              { await b = 1 ;
                b := (1 + 1)  %  2
              }
      }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} TickTock1 \.{=} 1 {\p@rparen}}%
 \@x{\@s{24.59} {\p@lbrace} t\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{57.03} {\p@lbrace} {\p@await} b \.{=} 1 {\p@semicolon}}%
\@x{\@s{66.61} b \.{:=} ( 1 \.{+} 1 )\@s{4.1} \.{\%}\@s{4.1} 2}%
\@x{\@s{57.03} {\p@rbrace}}%
\@x{\@s{24.59} {\p@rbrace}}%
\end{tlatex}
\end{twocols}
\end{display}
Since \,$(0+1)\,\%\,2$\, equals $1$, and \,$(1+1)\,\%\,$\, equals~0,
these two processes $TickTock0$ and $TickTock1$ are the same as
\popref{tick-tock-procs}{processes $Tick$ and $Tock$} of algorithm
$TickTock$---except for the labels, which don't appear in the
translations.  A comparison of the \tlaplus\ translations of the two
algorithms shows them to be equivalent.
 
\subsubsection{An \emph{N}-Valued Clock}

Algorithm $TickTock2$, like the equivalent algorithm $TickTock$,
describes a two-value clock.  We now generalize it to describe an
$N$-valued clock, for an arbitrary integer $N$ greater than~1.
Create a new specification named 
 \target{clockspec}%
$ClockSpec$ that begins with:
\begin{display}
\begin{twocols}[.35]
$\EXTENDS Integers$\V{.4}
$\CONSTANT N$ \\
$\ASSUME (N \in Nat) /\ (N > 1)$
\midcol
\verb|EXTENDS Integers| \V{.4}
\verb|CONSTANT N| \\
\verb|ASSUME (N \in Nat) /\ (N > 1)| 
\end{twocols}
\end{display}
We represent the clock by a variable $c$ that, in an execution, has
values lying in the set $0\dd (N-1)$; it initially equals~0.  The
PlusCal algorithm has the following \textbf{process} declaration,
which is the obvious generalization of the one in algorithm
$TickTock2$ above.
\begin{display}
\begin{nopcal}
process (Tick \in 0..(N-1)) 
      { t: while (TRUE)
              { await c = self ;
                c := (self + 1)  %  N 
              }
      }
\end{nopcal}
\begin{tlatex}
 \@x{ {\p@process} {\p@lparen} Tick \.{\in} 0 \.{\dotdot} ( N \.{-} 1 )
 {\p@rparen}}%
 \@x{\@s{24.59} {\p@lbrace} t\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
\@x{\@s{57.03} {\p@lbrace} {\p@await} c \.{=} self {\p@semicolon}}%
\@x{\@s{66.61} c \.{:=} ( self \.{+} 1 )\@s{4.1} \.{\%}\@s{4.1} N}%
\@x{\@s{57.03} {\p@rbrace}}%
\@x{\@s{24.59} {\p@rbrace}}%
\end{tlatex}
\end{display}
Add \popref{clockspec-ascii}{the \textsc{ascii} text of the algorithm}
to the module and run the translator on it.  Check that you haven't
made any error by having TLC check that the algorithm satisfies
the invariant $c \in 0\dd(N-1)$, for some value of $N$.

\subsubsection{An Implementation of the \emph{N}-Valued Clock}

The heart of the handshake algorithm is an implementation of the 
one-bit clock under the refinement mapping 
 \,$\ov{b}\;\deq\; (p + c) \,\%\,2$\,,
where the variables $p$ and $c$ have one-bit values that are each
modified by a single process.  We now generalize this to an
implementation of the $N$-valued clock by an array $ca$ of 1-bit
values.  To indicate how the implementation works, here are the values
of $ca$ with the corresponding values of \ov{c} for the refinement
mapping in the first few states of an execution, with $N=4$.
%
\begin{display}
\begin{tabular}{cc@{\ \ }c@{\ \ }c@{\ \ }c@{\hspace{2em}}c}
         & \underline{\strut$ca[0]$} & \underline{\strut$ca[1]$} 
                   & \underline{\strut$ca[2]$} & \underline{\strut$ca[3]$} & 
                  \underline{\ \strut\ov{c}\ }\V{.5}
State 1: &   0     &     0   &    0    &  0  & 0\V{.3}
State 2: &   1     &     0   &    0    &  0  & 1\V{.3}
State 3: &   1     &     1   &    0    &  0  & 2\V{.3}
State 4: &   1     &     1   &    1    &  0  & 3\V{.3}
State 5: &   1     &     1   &    1    &  1  & 0\V{.3}
State 6: &   0     &     1   &    1    &  1  & 1\V{.3}
State 7: &   0     &     0   &    1    &  1  & 2\V{.3}
State 8: &   0     &     0   &    0    &  1  & 3\V{.3}
 $\vdots$ 
\end{tabular}
\end{display}
%
Think of the $N$ processes with ids $0$, \ldots, $N-1$ arrayed
clockwise around a circle \popref{circle-of-n-procs}{like this}, so
process $(i+1)\,\%\,N$ follows process $i$.  The value of $ca[i]$ is
modified only by process~$i$, and is read only by~processes $i$ and
$(i+1)\,\%\,N$.  

Open a new specification named $HSClock$, and have it extend the $Integers$
module and declare the constant $N$ to be an integer greater than~1
as in \lref{clockspec}{module $ClockSpec$ above}.  Add the same definition
of $\oplus$ as in the two-phase handshake algorithm:
\begin{display}
\begin{twocols}[.4]
$a \oplus b == (a + b) \,\%\, 2$
\midcol
\verb|a (+) b == (a + b) % 2|
\end{twocols}
\end{display}
The $HSClock$ algorithm initializes $ca$ to be an array indexed by
$0\dd(N-1)$ with $ca[i]=0$ for all $i$ in $0\dd(N-1)$.
\begin{display}
\begin{tlatex}
 \@x{{\p@variable} ca \.{=} [ i \.{\in} 0 \.{\dotdot} ( N \.{-} 1 )
 \.{\mapsto} 0 ] {\p@semicolon}}%
\end{tlatex}
\end{display}
The code for process~0 differs from the code for processes 1, \ldots,~$N-1$.
\begin{display}
\begin{nopcal}
process (Proc0 = 0) 
  { t: while (TRUE) 
        { await ca[0]  =  ca[N - 1] ;
          ca[0]  :=  ca[0] (+) 1
        }
  }
   
process (Proc \in 1..(N-1)) 
  { t: while (TRUE) 
        { await ca[self]  #  ca[self - 1] ;
          ca[self]  :=  ca[self] (+) 1 
        }
  }
\end{nopcal}
\begin{tlatex}
\@x{ {\p@process} {\p@lparen} Proc0 \.{=} 0 {\p@rparen}}%
 \@x{\@s{8.2} {\p@lbrace} t\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
 \@x{\@s{32.43} {\p@lbrace} {\p@await} ca [ 0 ]\@s{4.1} \.{=}\@s{4.1} ca [ N
 \.{-} 1 ] {\p@semicolon}}%
\@x{\@s{42.01} ca [ 0 ]\@s{4.1} \.{:=}\@s{4.1} ca [ 0 ] \.{\oplus} 1}%
\@x{\@s{32.43} {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\@pvspace{8.0pt}%
 \@x{ {\p@process} {\p@lparen} Proc \.{\in} 1 \.{\dotdot} ( N \.{-} 1 )
 {\p@rparen}}%
 \@x{\@s{8.2} {\p@lbrace} t\@s{.5}\textrm{:}\@s{3} {\p@while} {\p@lparen}
 {\TRUE} {\p@rparen}}%
 \@x{\@s{32.43} {\p@lbrace} {\p@await} ca [ self ]\@s{4.1} \.{\neq}\@s{4.1} ca
 [ self \.{-} 1 ] {\p@semicolon}}%
\@x{\@s{42.01} ca [ self ]\@s{4.1} \.{:=}\@s{4.1} ca [ self ] \.{\oplus} 1}%
\@x{\@s{32.43} {\p@rbrace}}%
\@x{\@s{8.2} {\p@rbrace}}%
\end{tlatex}
\end{display}
Add \popref{hsclock-ascii}{the \textsc{ascii} text of the algorithm}
to the module and run the translator.  Examine the code and its
translation.  Observe that for $N=2$, the values of $ca[0]$ and
$ca[1]$ behave the same as the values of $p$ and $c$ in the original
$Handshake$ algorithm (the one with only a single label in each
process).

Type correctness of the algorithm is expressed by invariance of:
\begin{display}
\begin{twocols}[.4]
$ca \in [0\dd(N-1) -> \{0, 1\}]$
\midcol
\verb|ca \in [0..(N-1) -> {0, 1}]|
\end{twocols}
\end{display}
Have TLC check that this is indeed an invariant of the algorithm.

The algorithm executes the following sequence of actions: $Proc0$,
$Proc(1)$, $Proc(2)$, \ldots, $Proc(N-1)$, $Proc0$, $Proc(1)$,
\ldots\,.  It does this because, in every reachable state, either:
\begin{display}
\begin{itemize}
\item[HS1.] $ca[0] = ca[1] = \ldots = ca[N-1]$,  so only action $Proc0$
is enabled, or

\item[HS2.] $ca[0] = \ldots = ca[i-1] # ca[i] = \ldots = ca[N-1]$ for some
$i$ in $1\ldots(N-1)$, so only action $Proc(i)$ is enabled.
\end{itemize}
\end{display}
\begin{sloppypar}
We expect $HSClock$ to implement $ClockSpec$ under a refinement mapping
in which $Proc0$ implements $Tick(0)$ and $Proc(i)$ implements $Tick(i)$,
for $i$ in \mbox{$1\dd(N-1)$}.  From HS1 and HS2, this leads us to define
the refinement mapping by letting \ov{c} equal
\end{sloppypar}
\begin{display}
\begin{notla}
IF \E i \in 1..(N-1) : ca[i] # ca[i-1]
  THEN CHOOSE i \in 1..(N-1) : ca[i] # ca[i-1] 
  ELSE 0
\end{notla}
\begin{tlatex}
 \@x{ {\IF} \E\, i \.{\in} 1 \.{\dotdot} ( N \.{-} 1 ) \.{:} ca [ i ] \.{\neq}
 ca [ i \.{-} 1 ]}%
 \@x{\@s{8.2} \.{\THEN} {\CHOOSE} i \.{\in} 1 \.{\dotdot} ( N \.{-} 1 ) \.{:}
 ca [ i ] \.{\neq} ca [ i \.{-} 1 ]}%
\@x{\@s{8.2} \.{\ELSE} 0}%
\end{tlatex}
\end{display}
In module $HSClock$, define $cBar$ 
  \marginpar{\popref{hsclock-cbar-ascii}{\textsc{ascii} 
     version of definition and \textsc{instance}
     statement}}%
to equal this expression and add:
\begin{display}
$CS == \INSTANCE ClockSpec \WITH c <- cBar$
\end{display}
(Note that there is an implicit \textsc{with} clause $N <- N$ that
substitutes the declared constant $N$ of module $HSClock$ for the
declared constant $N$ of module $ClockSpec$.)  Check that algorithm
$HSClock$ implements algorithm $ClockSpec$ under this refinement
mapping by having TLC check that specification $Spec$ of $HSClock$
satisfies the property $CS!Spec$, for a model with a small value of~$N$.

\begin{hquestion}{hsclock-refinement}
Prove that $HSClock$ implements $ClockSpec$ under this refinement
mapping by proving \lref{r1r2}{conditions R1 and R2} for a suitable
invariant $Inv$.
\end{hquestion}



\subsubsection{Round-Robin Synchronization}


\begin{question}
(a)~Add a \textbf{skip} statement and labels to algorithm $ClockSpec$
to obtain a specification $RoundRobin$ of round-robin synchronization
that generalizes the specification of alternation by
\popref{altspec}{algorithm $AltSpec$} we wrote in
\lref{\xlink{alt-revisited}}{Section~\xref{alt-revisited}}.

(b)~Modify algorithm $HSClock$ to obtain a generalization of the
two-phase handshake protocol that implements specification
$RoundRobin$ under a suitable refinement mapping.  Use TLC to check
your answer.
\end{question}

%try
\end{comment}

\end{document}

%% BEING SUSPICIOUS OF MODEL-CHECKING SUCCESS
A cardinal rule of model checking is: 
\begin{display}
  \tindex{1}{suspicion of success}%
\emph{Be suspicious of success.}
\end{display}
It's very easy for a mistake to mean that your invariant isn't really
checking anything.  Usually, this happens because the specification
doesn't allow the system to reach any interesting states.  For
example, this invariant is trivially true if the algorithm were never
to reach a state with $x=y$.  You should always check your invariant
by modifying it slightly in a way that should cause it to be violated
by some behavior.  In this example, we might just change the
right-hand side of the implication in some way---for example,
replacing $GCD(M, N)$ by 42.  Do this and check that TLC finds the
error (assuming you haven't happened to choose values of $M$ and $N$
whose gcd is 42).

Another\target{main-coverage} place 
  \tindex{1}{coverage}%
to look for errors is in the \emph{Coverage} table of the
\emph{Statistics} section of the \emph{Model Checking Results} page.
After running TLC, it shows a list of lines, each with a location and
count.  A location is part of the next-state action.  Clicking on a
location shows you the part of the next-state action the line is
describing.  The count is the number of times that TLC evaluated that
part of the next-state action when computing all the behaviors allowed
by the specification.  (In this case, for each value of $M$ and $N$,
there is only a single behavior that satisfies the spec of module
$Euclid$.)  A count that seems too small---especially a count of
0---indicates a possible problem.  A count of 0 indicates that part of
the spec is not contributing to the set of allowed behaviors, and
removing it does not change the spec---at least, not for that choice
of constant parameters.  At best, this means that the constant
parameters you chose don't cause the entire next-state action to be
checked.  At worst, it means that there's an error in your spec.

==================================================================

%% WRITING INFORMAL HYPERTEXT PROOFS

    \ctindex{1}{proof!hypertext}{proofs-hypertext}%
    \tindex{1}{hypertext proof}%
    \vspace{-\baselineskip}%
\subsubsection{Writing Informal Hypertext Proofs}

\tlaplus\ has constructs for writing formal structured proofs.  The
  \tindex{4}{TLAPS}%
TLAPS proof system can be used to check such proofs.
\popref{euclid-inv-2}{Click here for the \textsc{ascii} version} of a
\tlaplus\ proof of theorem $Induction$.  Copy it and paste it at the
end of the $Euclid$ module, and save the module (which should parse
it).  This proof was checked with TLAPS, but don't try to understand
it.  \tlaplus\ proofs are explained in \rref{proof}{top}{The \tlaplus\
Proof Track}, but here I just want you to view it as hypertext.

There is a {\small $\ominus$} next to the theorem and next to each
step.  Clicking on the {\small $\ominus$} next to a step hides the
step's proof and changes the {\small $\ominus$} to a {\small
$\oplus$}.  Clicking on the {\small $\oplus$} undoes the effect of
clicking on the {\small $\ominus$}.
%
Right-clicking on a step provides a menu with a section containing the
following commands for hiding and revealing parts of the proof of the
step at which the 
  \popref{cursor}{cursor} is positioned.  These commands can also be
executed with the indicated pair of keystrokes.  If you just type
\textsf{control+g} (the first keystroke) and wait a second, you will
see a list of all the commands telling you what keystroke you should
type next.
\begin{description}
\item[Show Current Subtree]     \textsf{control+g} \textsf{control+s} \\
Reveals the proof of the step. 

\item[Hide Current Subtree]     \textsf{control+g} \textsf{control+h} \\
Hides the proof of the step. 

\item[Show Children Only]     \textsf{control+g} \textsf{control+c} \\
Reveals the top level of the step's proof. 

\item[Focus on Step]     \textsf{control+g} \textsf{control+f} \\
Hides everything except the top level of the step's proof and the
siblings of (steps at the same level as) the step and of every
ancestor of that step in the proof.
\end{description}
Now observe that you can replace any of the \texttt{BY} statements
with \texttt{BY TRUE} and the module still parses correctly.  For example,
replace 
\begin{verbatim}
    <3>1. GCD(y, x) = GCD(y', x')
      BY <1>1, <1>2, GCD3 DEF Inv, TypeOK
\end{verbatim}
with
\begin{verbatim}
    <3>1. GCD(y, x) = GCD(y', x')
      BY TRUE
\end{verbatim}
and save the module.  The trick to writing informal hypertext proofs
is to insert the informal proof between the \texttt{BY} and the
\texttt{TRUE}\@.  The Toolbox and parser will then treat everything from
\texttt{BY} through \texttt{TRUE} as the proof of the step.  For
example, change that part of the proof to
\begin{verbatim}
    <3>1. GCD(y, x) = GCD(y', x')
                                                          BY(* 
       Proof: <1>1.1 (assumption Inv) and the definitions of Inv 
       and TypeOK imply that x and y are in Nat \ {0}, so <3>1 
       follows from case assumption <1>2 and GCD3 (substituting 
       y for m and x for n).                              *)TRUE
\end{verbatim}
and save the module.  You can now hide and reveal parts of the
informal proof.  Try different ways of formatting your proofs until
you find the one you like best.  It's not ideal, but it should work
pretty well.

=============================================================================


%% AN INTRODUCTION TO TEMPORAL LOGIC


  \ctindex{1}{temporal logic@\icmd{target}{temp-logic}{temporal logic}}{temporal-logic}%
  \ctindex{1}{logic!temporal|see{\icmd{lref}{temp-logic}{temporal logic}}}{logic-temporal}%
\subsection{An Introduction to Temporal Logic} \xlabel{intro-temporal-logic}


We have been describing systems with two formulas: an initial
predicate $Init$ and a next-state action $Next$.  It is more elegant,
and sometimes more useful, to combine these two formulas into a single
formula.  In fact, you may have observed that when you created a model
to check the algorithm of module $PCalEuclid$, the Toolbox created the
model with the behavior spec equal to the temporal formula $Spec$
instead of being specified by $Init$ and $Next$.

To write the specification as a single formula, we need temporal
logic.  Temporal logic can be a forbidding, esoteric subject.
Fortunately, the temporal logic we need is very simple.

\subsubsection{Temporal Formulas}
\xlabel{main:temporal-formulas}

The initial predicate $Init$ is a 
  \ctindex{1}{state!formula}{state-formula}%
  \ctindex{1}{formula!state}{formula-state}%
\emph{state formula}, meaning that
it is a predicate on states---in other words, a formula that is true or false
of a state.  The next-state action $Next$ is an 
  \ctindex{1}{action!formula}{action-formula}%
  \ctindex{1}{formula!action}{formula-action}%
\emph{action formula},
meaning that it is a predicate on steps (pairs of states).  A
temporal-logic formula, called a 
  \tindex{1}{temporal formula}%
  \ctindex{1}{formula!temporal}{formula-temporal}%
\emph{temporal formula} for short, is true or false of a behavior (a
sequence of states).  As with state and action formulas, we often say
that a behavior \emph{satisfies} a temporal formula $F$ to mean that
$F$ is true of (or on) the behavior.

We can consider a state formula $S$ to be an action formula that
depends only on the first state of a step.  That is, $S$ is the
action formula that is true of the step $s -> t$ iff it is true (as a
state formula) of the state $s$.  We consider an action
formula $A$ 
to be the temporal formula that is true of a behavior
  \[ s_{1} -> s_{2} -> s_{3} -> \ldots \]
iff $A$ is true (as an action formula) of the first step
$s_{1}->s_{2}$ of the behavior.  Thus, a state formula $S$ is the
temporal formula that is true of this behavior if $S$ is true (as a
state formula) of the first step $s_{1}$ of the behavior.

The meaning of the ordinary logical operators like $/\ $ and $=>$
on temporal formulas is straightforward.  For example, if $F$ and $G$
are temporal formulas, then $F/\ G$ is the temporal formula that is
true of a behavior $\sigma$ iff both $F$ and $G$ are true of
$\sigma$.  Similarly, $F=>G$ is true of $\sigma$ iff $F$ is false on
$\sigma$ or $G$ is true on~$\sigma$. 

The only temporal formulas that you are likely to use are ones that
can be written with action formulas (including state formulas), the
ordinary logical operators, and the single temporal operator 
   \ctindex{1}{+3v@\mmath{\icmd{Box}} (always)}{+3v}%
$[]$,
which is written \texttt{[\,]} in \textsc{ascii}.
For any temporal formula $F$, the temporal formula $[]F$ is defined
to be true of the behavior
  \[ s_{1} -> s_{2} -> s_{3} -> \ldots \]
iff $F$ is true of all suffixes 
  \marginpar{A behavior is a suffix of itself.  
             (It is not a \emph{proper} suffix of itself.)}
of this behavior---that is,
for all the behaviors
  \[ \begin{noj}
     s_{1} -> s_{2} -> s_{3} -> \ldots \\
     s_{2} -> s_{3} -> s_{4} -> \ldots \\
     s_{3} -> s_{4} -> s_{5} -> \ldots \\
     \mbox{etc.}
     \end{noj}
  \]
We can consider $s_{1}->s_{2}->s_{3}->\ldots$ to be the behavior
starting now, $s_{2}->s_{3}->\ldots$ to be the behavior starting one
time unit later, $s_{3}->s_{4}->\ldots$ to be the behavior starting
two time units later, and so on.  
 \marginpar{\popref{time-vs-step}{Don't take this notion 
   of time seriously.}}
Thus, $[]F$ is true now iff $F$ is
true starting now and at all future times.  We therefore sometimes say
that $[]F$ asserts that $F$ is always true or henceforth true, and
we can read $[]$ as 
  \tindex{1}{always}%
\emph{always} or 
  \tindex{1}{henceforth}%
\emph{henceforth}.  (However, I usually just read $[]$ as \emph{box}.)


A state formula $S$ is true of the behavior
$s_{1}->s_{2}->s_{3}->\ldots$ iff it is true of the state $s_{1}$; it
is true of the behavior $s_{2}->s_{3}->s_{4}->\ldots$ iff it is true
of state $s_{2}$, and so forth.  Hence, for a state formula $S$, the
temporal formula $[]S$ is true of a behavior $s_{1}-> s_{2}-> \ldots$
iff $S$ is true of all states $s_{i}$ of the behavior.  Similarly,
for an action formula $A$, the temporal formula $[]A$ is true of
this behavior iff $A$ is true of all steps $s_{i}->s_{i+1}$ of the
behavior.  

\smallskip

\textbf{Warning } The following paragraph begins by lying to you, but
it soon tells the truth.  

\smallskip

Consider 
the temporal formula 
 $Init /\ []Next$.  What does it assert about a behavior $s_{1}->
s_{2}->s_{3}->\ldots$\,?  It is a conjunction, so it is true of this
behavior iff both $Init$ and $[]Next$ are true of the behavior.  Since
$Init$ is a state formula, it is true of the behavior iff it is
true of state $s_{1}$.  Since $Next$ is an action formula, $[]Next$ is
true of the behavior iff $Next$ is true of every step $s_{i}->s_{i+1}$
of the behavior.  Thus, $Init /\ []Next$ is true of a behavior iff the
behavior's first state satisfies $Init$ and every step of the behavior
satisfies $Next$.  In other words, a behavior satisfies formula $Init
/\ []Next$ iff it is a behavior of the system described by the initial
predicate $Init$ and the next-state action $Next$.  Thus, we can define
$Spec$, the temporal formula that specifies (all behaviors of) the
system by
 \[ Spec == Init /\ []Next \] 
This is a lie.  Change the beginning of the algorithm
back to \textbf{-{}-algorithm} in
module $PCalEuclid$, translate the algorithm, and look at the \tlaplus\
translation.  It defines the specification $Spec$ by
 \[ Spec == Init /\ [][Next]_{vars}\]
where $vars$ is defined to be the tuple of all the algorithm's
variables, and $[][Next]_{vars}$ is written in \textsc{ascii}
as \verb|[][Next]_vars|.
If you change the definition to $Init /\ []Next$ and save
the module, you will get a parsing error telling you that $[]Next$ is
not a syntactically correct \tlaplus\ formula.

In general, the temporal formula asserting that a behavior satisfies
the initial predicate $Init$ and next-state action $Next$ is
 \[ Init /\ [][Next]_{\ldots} \]
where $\ldots$ is the tuple of all system variables, written in any
order.  It doesn't matter if we define an identifier like $vars$ to
equal that tuple or just write the tuple explicitly.  For now, you
should simply pretend $[][Next]_{vars}$ is $[]Next$.  To help you, I
will write it as $[]\gssubvars{Next}$ (which is typed
\verb|[]|{\gray\verb|[|}Next{\gray\verb|]_vars|}
in \textsc{ascii}).

\bigskip

The formula $Init /\ []\gssubvars{Next}$ asserts of a behavior
$\sigma$ that $\sigma$ starts in a state satisfying $Init$, and that
every step of $\sigma$ satisfies \gssubvars{Next}.  It does not assert
that $\sigma$ contains any steps.  If a behavior satisfies this
formula, then any nonempty prefix of that behavior also satisfies it.

If you change the beginning 
  \target{euclid-fairness}%
of Euclid's algorithm back to
\textbf{-{}-fair~algorithm} and run the PlusCal translator, you will
see that the definition of the specification $Spec$ becomes 
 \[ Spec == 
      \begin{conj}
        Init /\ []\gssubvars{Next} \\
       \WF_{vars}(Next)
      \end{conj}
 \]
The additional conjunct $\WF_{vars}(Next)$, which is written
\verb|WF_vars(Next)| in \textsc{ascii}, is a temporal formula that is
true of a behavior $\sigma$ (approximately) iff $\sigma$ does not end
in a state from which it is possible to take a step that satisfies
action $Next$.  In other words, $\WF_{vars}(Next)$ asserts that the
execution does not stop if the algorithm can take an additional step.
If $\sigma$ is any behavior that satisfies
 $Init /\ []\gssubvars{Next}$, 
then no proper prefix $\tau$ of $\sigma$
satisfies $Spec$.  This is because if $\tau$ consists of the first $k$
states of $\sigma$, then the step from the last state of $\tau$ to the
$(k+1)$\st\ state of $\sigma$ satisfies $Next$ (because $\sigma$
satisfies $[]\gssubvars{Next}$).

\subsubsection{Properties of a Specification}

We can express properties of a system as temporal logic formulas.  
More precisely, a 
  \ctindex{1}{property!is temporal formula}{property-temporal-formula}%
property \emph{is} a temporal formula.  

One 
  \ctindex{1}{invariant!as temporal property}{invariant-temp-prop}%
kind of property we have considered so far is invariance.  The
following chain of reasoning leads to a formalization of what it means
for a formula $Inv$ to be an invariant of a system with specification
$Spec$.
\begin{display}
$Inv$ is an invariant of $Spec$\V{.3}
\s{.5}iff\s{.5}\begin{minipage}[t]{.85\textwidth}
   $Inv$ is true in every state of every behavior 
  satisfying $Spec$%
             \vs{.6}\end{minipage}\\
\s{.5}iff\s{.5}\begin{minipage}[t]{.85\textwidth}
    $[]Inv$ is true of every behavior 
  satisfying $Spec$%
             \vs{.6}\end{minipage}\\
\s{.5}iff\s{.5}\begin{minipage}[t]{.85\textwidth}
      $Spec => []Inv$ is true of every behavior
             \vs{.6}\end{minipage}
\end{display}
Hence, $Inv$ is an invariant of a system with specification $Spec$ iff
$Spec => []Inv$ is true of all behaviors.  A temporal formula that is
true of all behaviors is called a \emph{theorem}.  Thus, the invariance
of $Inv$ is expressed by the following theorem:
\begin{display}
$\THEOREM Spec => []Inv$
\end{display}
Instead of saying that this theorem is true,
  \marginpop{truth-vs-provability}{Truth versus provability.}
 we usually 
say that the formula $Spec => []Inv$ is true.


The property that $Inv$ is an invariant is expressed by the temporal
formula $[]Inv$.  The specification $Spec$ satisfies this property
iff the $Spec => []Inv$ is true, meaning that it is satisfied by
all behaviors.

In general, a specification $Spec$ satisfies a property $P$ iff
$Spec=>P$ is a theorem.  You can have TLC check a property by adding
it to the \textsf{Properties} list in the \textsf{What to check?}
section of the model's \textsf{Model Overview} page.  Telling TLC to
check the property $[]Inv$ in this way is equivalent to adding $Inv$
to the model's list of \textsf{Invariants}.



Another property we encountered is the formula $Termination$, defined
in the PlusCal translation of Euclid's algorithm.  I stated that it
asserts that the algorithm eventually terminates.  If you examine the
translation in module $PCalEuclid$, you will see that the definition
is%
  \ctindex{1}{+3x@\mmath{\icmd{Diamond}} (eventually)}{+3x}%
  \[ Termination == <>(pc = "Done") \]
where $<>$ is written in \textsc{ascii} as \texttt{<\s{.07}>}.  The
operator $<>$ is defined to equal $~[]~$, meaning that $<>F$ equals
$~[]~F$ for any formula $F$.  So, let's figure out what $Termination$
means.
\begin{display}
\begin{tabbing}
$<>(pc = "Done")$ is true of a behavior $\sigma$\V{.3}
\s{1}\=iff\s{.5}\=
   $<>(pc = "Done")$ is true of $\sigma$ \s{8}\= 
      \small[definition of $Termination]$\+\kill
iff\>
    $~[]~(pc = "Done")$ is true of $\sigma$
    \>\small[Definition of $<>$]\s{-10}\vs{.6}\\
iff\>
    $~[](pc # "Done")$ is true of $\sigma$
    \>\small[$~(a=b) \;\equiv\; (a#b)$]\s{-10}\vs{.6}\\
iff\>
    $~$\,(\,($pc # "Done")$ is true of every state of $\sigma$)
      \>\small [Definition of $[]$]\s{-10}\vs{.6}\\
iff\>
    $~(pc # "Done")$ is true of some state of $\sigma$
     \>\small [$~(\A s : P) \; \equiv \; (\E s : ~P)$]\s{-10}\vs{.6}\\
iff\>
    $pc = "Done"$ is true of some state of $\sigma$
     \>\small [$~(a#b) \;\equiv\; (a=b)$]\s{-10}
\end{tabbing}
\end{display}
In other words, $<>(pc = "Done")$ asserts that eventually $pc =
"Done"$ is true, which means that eventually the algorithm terminates.
In general, for any state formula $P$, the formula $<>P$ asserts of a
behavior $\sigma$ that $P$ is true in some state of $\sigma$.  

More generally, for any temporal formula $F$, formula $<>F$ is true of
a behavior $\sigma$ iff $F$ is true of some suffix of $\sigma$.  We
often read $<>F$ as 
  \tindex{1}{eventually}%
\emph{eventually} $F$, where we consider
\emph{now} to be a special case of \emph{eventually}, so \emph{true now}
implies \emph{eventually true}. 

\subsubsection{Temporal Logic Reasoning} \xlabel{temporal-logic-reasoning}

When $Spec$ is the formula $Init /\ []\gssubvars{Next}$, we saw
in 
  \sref{main}{\xlink{main:sec-rigorous}}{Section~\xref{main:sec-rigorous} above}
that we prove invariance by proving the following two theorems
\begin{display}
$\THEOREM Init => Inv$\V{.3}
$\THEOREM Inv /\ \gssubvars{Next} => Inv'$
\end{display}
We express this reasoning as the following proof rule:\target{main:INV}
 \[\mbox{INV.} \; \; 
       \proofrule{Init => Inv, \; Inv /\ \gssubvars{Next} => Inv'}%
                 {Init /\ []\gssubvars{Next} => []Inv}
 \]
(Remember that, for now, you should just pretend that the
\gssubvars{\;\;} is not there.)

In general, a 
  \tindex{1}{proof rule}%
  \tindex{1}{rule, proof}%
proof rule of the form
 \[ \mbox{PR.}\;\;\proofrule{F_{1}, \ldots, F_{n}}{G}
 \]
means that, to prove
 \[ \THEOREM G\]
it suffices to prove the $n$ separate theorems
 \begin{display}
$\THEOREM F_{1}$, 
\ldots\,,
$\THEOREM F_{n}$
\end{display}
This proof rule is \emph{not} equivalent to the theorem:
\begin{display}
$\THEOREM PRThm == F_{1} /\ \ldots \, /\ F_{n} \;=>\; G$
\end{display}
A theorem asserts that a formula is true for all behaviors.
We can therefore write the assertions stated by the proof rule
and the theorem informally as follows:
 \[ \begin{noj2}
    \mbox{PR}: & (\A \sigma : F_{1}) /\ \ldots \, /\ (\A \sigma : F_{n})
                   \; => \; (\A \sigma : G) \V{.3}
    PRThm: & \A \sigma : (F_{1} /\ \ldots \, /\ F_{n}
                   \; => \; G )
    \end{noj2}
 \]
Make sure you understand why these are two different assertions.
\begin{aquestion}{proofrule-vs-thm}
Prove that $PRThm$ is stronger than (implies) PR.
\end{aquestion}
%
The difference between the proof rule PR and the theorem $PRThm$ is
the most difficult part of temporal logic.  You need to understand it
to avoid making errors in temporal-logic reasoning.  Once you do
understand it, you should have no trouble with temporal logic proofs.

=============================================================================

%% OLDER GARBAGE

%



Red1--Red4 are defined below the statement of the theorem.

\bigskip\noindent
\textbf{Simple PlusCal Reduction Theorem} Let \MF\ be a PlusCal algorithm
with a process $\Pi$  containing the consecutive statements
\begin{display}
$r_{1}$: $R_{1}$ ; \ $r_{2}$: $R_{2}$ ; \ \ldots\,\  $r_{n}$: $R_{n}$
\end{display}
where each $R_{i}$ has no labels, and assume that replacing these statements
with 
\begin{display}
$\red r_{1}$: $R_{1}$ ; \ $R_{2}$ ; \ \ldots\,\ $R_{n}$
\end{display}
produces a legal PlusCal algorithm \rd{\MF}.  Assume that all the
actions $r_{i}$ of \MF\ have the same fairness condition, and that
each action of \rd{\MF} has the same fairness condition as the
corresponding action of \MF, where action $\red r_{1}$ of \rd{\MF} has
the same fairness condition as the actions $r_{i}$ of \MF. If Red1 and
Red2 hold, then for every behavior $\sigma$ of \MF\ there is a
behavior \rd{\sigma} of \rd{\MF} that is obtained from $\sigma$ by
replacing each execution of $r_{i}$ by a stuttering step, for $i#k$,
and replacing each execution of $r_{k}$ by an execution of action
$\red r_{1}$ of \rd{\MF}.  Moreover, for any refinement mapping
$x_{1}<-\ov{x_{1}}$, \ldots, $x_{m}<-\ov{x_{m}}$, if Red4 holds, then
$\sigma$ and \rd{\sigma} are mapped to the same behavior under this
refinement mapping.

\bigskip\noindent
%
\textbf{Definition } Two actions \emph{do not interact} iff
(i)~executing them in either order produces the same result and
(ii)~executing either of them cannot 
enable or disable the other.

\begin{describe}{\textbf{Red1}}
\item[\textbf{Red1}] There is a $k$ in $1\dd n$ such that for each
$i#k$, action $r_{i}$ does not interact with any action of any process
other than $\Pi$.
\end{describe}

\begin{describe}{\textbf{Red2}}
\item[\textbf{Red2}] For every $i$ in $(k+1)\dd n$, action $r_{i}$ is
enabled whenever control in process $\Pi$ is at $r_{i}$.
\end{describe}

\begin{describe}{\textbf{Red4}}
\item[\textbf{Red4}] For each $i#k$ and each $j$, action $r_{i}$
leaves \ovs{x_{j}} unchanged.
\end{describe}

\noindent
Note: the theorem implies that if Red1--Red4 hold, then if \rd{\MF}
implements some specification under the refinement mapping, then \MF\
also implements that specification under the same refinement mapping.
The invariance of a formula $Inv$ can be expressed as implementation
of a simple spec under a refinement mapping $x<-Inv$, so invariance is
is a special case of refinement.




XXXXXXXXXXXXXXXXXXXXXXX

However, $\rho$ and $\sigma$ need not be equivalent because:
\begin{itemize}
\item The values of $t$ and $p$ can change in different
steps of $\rho$, while they change in the same step of $\sigma$.

\item $pc["P"]$ (which describes the producer's control state) can
assume the values $"p3b"$ and $"p3c"$ in states of $\rho$, but not
states of $\sigma$.
\end{itemize}
Hence, at best, rot-equivalence can mean equivalent if we ignore the
variables $t$ and the $pc$.  If it does, then the following reasoning
allows us to deduce the correctness of algorithm $\mathcal{F}$ from
the correctness of algorithm $FGBBuf$.
\begin{enumerate}

%1
\item It suffices to assume $\rho$ is an arbitrary behavior of
$\mathcal{F}$ and show that there is a behavior $\tau$ of $FGBBuf$ such
that \ov{\rho} is equivalent to \ov{\tau}, where \ov{\rho} and
\ov{\tau} are defined by the refinement mapping $ch <- chBar$.
\begin{proof}
\pf\ Correctness means implementing the bounded channel under
this refinement mapping.
\end{proof}

%2
\item Let $\sigma$ be a behavior of $\mathcal{C}$ that is
rot-equivalent to $\rho$.
\begin{proof}
\pf\ The rule of thumb implies the existence of $\rho$.
\end{proof}

%1->%3

\item If we ignore the values of $pc$ and $t$, statement $p3$ of
$FGBBuf$ is the same as statement $p3$ of algorithm $\mathcal{C}$.
\begin{proof}
\pf\ Obvious
\end{proof}

%2->%4
\item The behavior $\sigma$ of $\mathcal{C}$ is rot-equivalent
to a behavior $\tau$ of $FGBBuf$.
\begin{proof}
\pf\ By 3.
\end{proof}

%3->5
\item $\rho$ is rot-equivalent to $\tau$
\begin{proof}
\pf\ By 2 and 4.
\end{proof}

%4->6
\item \textsc{qed}
\begin{proof}
\pf\ By 1, since the value of $chBar$ does not depend on $t$ or $pc$,
which implies \ov{\rho} is equivalent to \ov{\tau} by 5, because we
are assuming assumption that rot-equivalent means equivalence when $t$
and $pc$ are ignored.
\end{proof}
\end{enumerate}
I will not try to define precisely what rot-equivalence means in the
rule of thumb.  Instead, I will describe how $\sigma$ is constructed
from $\rho$.  


For example, if $i<k$

To construct $\mu$



   \sref{main}{main:invariance-as-refinement}{Question~\ref{invariance-as-refinement}}


XXXXXXXXXXXXXXXXXXXX
Do we need to refine the 

\medskip

When you have written a specification and found no errors, you need to
determine if you should write a finer-grained specification.  To do
that, you should first decide what are the smallest atomic operations
you want to consider.  For the bounded buffer algorithm, I will take a
single access (a read or write) of $p$ or $c$ to be atomic.  We could
certainly imagine a finer-grained algorithm.  However, the purpose of
the bounded buffer algorithm is to implement the bounded channel under
some suitable refinement mapping.  I don't know how to do that if
accessing $p$ and $c$ are not atomic actions.  This means that if I
want to use the bounded buffer algorithm to implement a concrete
system described by the bounded channel specification, my
implementation must ensure that it implements the reads and writes of
those variables so they act like atomic actions.  I discuss below how
that can be done.

If we assume that a single access of $p$ is an atomic action, we can
refine the producer's atomic action $p3$ into the two atomic actions
$p3a$ and $p3b$ show above.  Do we have to do this?  We argue as
follows that this is not necessary.  Let $\mathcal{F}$ be a
fine-grained algorithm in which the single atomic action $p3$ has been
split into the separate atomic actions $p3a$ and $p3b$, and other
atomic actions may similarly have been refined.  Let $\mathcal{C}$ be
the (slightly) coarser-grained algorithm obtained from $\mathcal{F}$
by combining $p3a$ and $p3b$ into the single atomic action $p3$.  I
will show that if $\mathcal{C}$ is correct, then $\mathcal{F}$ is also
correct.  Applying the same reasoning to other actions besides $p3$ of
the algorithm of module $PCalBoundedBuffer$, we can show that there is
no need to further refine that algorithm.

Correctness of the bounded buffer means that it implements the bounded
channel under the refinement mapping $ch<-chBar$.  
  \sref{main}{main-refinement}{Recall what this means.}
Implementation under this refinement mapping asserts that, for any
behavior $\sigma$ satisfying the bounded buffer specification, the
behavior \ovs{\sigma} satisfies the bounded channel specification,
where \ovs{\sigma} is the behavior each of whose states \ovs{s_{i}} is
obtained from the corresponding state $s_{i}$ of $\sigma$ by letting
the value of $ch$ in state \ovs{s_{i}} equal the value of $chBar$ in
state $s_{i}$.  (Remember that $chBar$ is the formula we had been
calling \ovs{ch}.)

I will now show that for every behavior $\rho$ of the finer-grained
grained algorithm $\mathcal{F}$, there is a behavior $\sigma$ of the
coarser-grained algorithm $\mathcal{C}$ such that \ovs{\rho} and
\ovs{\sigma} are the same except for stuttering steps.  Because
specifications are insensitive to stuttering, this implies that
\ovs{\rho} satisfies the bounded channel specification iff
\ovs{\sigma} does.  Hence, if $\mathcal{C}$ implements the bounded
channel, then so does~$\mathcal{F}$.

Here is a sketch of the argument.  For now, we ignore liveness and
assume that $\mathcal{F}$ and $\mathcal{C}$ just have the form 
 $Init /\ [][Next]_{vars}$.
Let's write 
 \[s\action{A}t\]
to mean that $s->t$ is a step satisfying
action $A$.  Consider a behavior $\rho$ of algorithm $\mathcal{F}$ and
a $p3a$ step in that behavior.  A $p3b$ step must come between any two
$p3a$ steps of $\rho$.  Hence, if that $p3a$ step is not immediately
followed by a $p3b$ step, then it is followed by a step $C$ that is
either a consumer step or a stuttering step, as in
 \[ \ldots\  r_{i} \action{p3a} r_{i+1} \action{C} r_{i+2}\ \ldots
 \]
Recall that $p3a$ is the action $t := p\oplus 1$.  A consumer action
or a stuttering step does not modify the value of $p$, and it does not
read or modify the value of $t$.  Hence the actions $p3a$ and $C$
actions commute---meaning that executing $p3a$ and then $C$ has
the same effect as executing $C$ and then
executing $p3$.  This implies that there is a state $t_{i+1}$
such that replacing
  \[ r_{i} \action{p3a} r_{i+1} \action{C} r_{i+2}
  \]
by 
  \[ r_{i} \action{C} t_{i+1} \action{p3a} r_{i+2}
  \]
in the behavior $\rho$ produces a behavior $\mu$ that is also a
possible behavior of $\mathcal{F}$, since each of its steps satisfies
the initial predicate and next-state action of $\mathcal{F}$.
(Remember that, for now, we are assuming that $\mathcal{F}$ and
$\mathcal{C}$ have no liveness assertion.)

The value of $chBar$ depends only on the values of $p$, $c$, and
$buf$, none of which are changed by action $p3a$. Hence, 
$\ov{r_{i+1}}=\ov{r_{i}}$ and $\ov{r_{i+2}}=\ov{t_{i+1}}$.  This implies
that
 \[ \begin{noj3}
    \ov{\rho} & \;=\; & \ldots\  \ov{r_{i}} -> \ov{r_{i}} 
                -> \ov{r_{i+2}}\ \ldots \V{.3}
    \ov{\mu} & \;=\; & \ldots\  \ov{r_{i}} -> \ov{r_{i+2}} 
                  -> \ov{r_{i+2}}\ \ldots 
    \end{noj3}
 \]
Therefore, \ovs{\rho} and \ovs{\mu} are the same except for stuttering
steps.

By repeating this procedure, we can therefore move every $p3a$ action
in the behavior $\rho$
to the right until it is followed immediately by a $p3b$
action---except perhaps for a $p3a$ action not followed by any $p3b$
action, which can be moved to the right over all other actions and
thereby removed from the behavior.  In this way, 
we construct a new
behavior $\tau$ of $\mathcal{F}$ in which every $p3a$ action is
immediately followed by a $p3b$ action, such that $\ovs{\rho}$
and $\ovs{\tau}$ are the same except for stuttering steps.

We now construct the behavior $\sigma$ from $\tau$ by replacing each
pair of steps
 \[ r_{i} \action{p3a} r_{i+1} \action{p3b} r_{i+2}\]
with the single step
  \[ s_{i} \action{p3} r_{i+2}\]
where state $s_{i}$ differs from $r_{i}$ only by having $pc["P"]$
equal to $p3$ instead of $p3a$.  The behavior $\sigma$ is a behavior
of algorithm $\mathcal{C}$.  Because a $p3a$ action leaves $chBar$
unchanged, $\ov{r_{i}}$ equals \ov{r_{i+1}}; and $\ov{r_{i}}$ equals
\ov{s_{i}} because $chBar$ does not depend on $pc$.  Therefore
\ovs{\tau} and \ovs{\sigma} differ only by stuttering steps, which
implies that \ovs{\rho} and \ovs{\sigma} differ only by stuttering
steps.

For every behavior $\rho$ of $\mathcal{F}$, I have constructed a
behavior $\sigma$ of $\mathcal{C}$ such that \ovs{\rho} and
\ovs{\sigma} differ only by stuttering steps.  This shows that 
to prove that $\mathcal{F}$ implements the bounded channel under
the refinement mapping $ch<-chBar$, it suffices to show that
$\mathcal{C}$ does.

\medskip

It's easy to see that the same argument could be used if the single
atomic action $p := p \oplus 1$ were split into more than two
actions---for example, if it were split into the three separate actions.
\begin{display}
$p3aa$: $t := p$; \ \ \ $p3ab$: $t := t \oplus 1$; \ \ \ $p3b$: $p := t$
\end{display}
For any behavior $\rho$ of the fine-grained algorithm, we could move
$p3aa$ and $p3ab$ actions to the right to obtain a behavior $\tau$ of
this algorithm in which $p3aa$, $p3ab$, and $p3b$ actions are
consecutive with \ovs{\rho} and \ovs{\tau} the same except for
stuttering actions.  By combining the three consecutive actions into a
single $p3$ action, we then obtain a behavior $\sigma$ of the
coarser-grained algorithm such that \ovs{\tau} and \ovs{\sigma} differ
only by stuttering steps. 